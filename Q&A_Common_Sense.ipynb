{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import random as rd\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Data\n",
    "Parsing xml train and dev data\n",
    "Tokenising text/questions/answers, getting rid of punctuation and upper cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml_train = open('MCScript/train-data.xml').read()\n",
    "xml_dev = open('MCScript/dev-data.xml').read()\n",
    "xml_test = open('MCScript/test-data.xml').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to preprocess data\n",
    "\n",
    "punctuation_and_extra = set(list('!\"#$%&\\'()*+,-./:;<=>? @[\\\\]^_`{|}~£'))\n",
    "punctuation_and_extra.update([\"'s\", \"n't\", \"``\", \"''\", \"'ll\", \"'m\", \"'d\", \"'ve\", \"wa\"])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokenised_text = []\n",
    "    for word in tokens:\n",
    "        if \":\" in word: #to deal with time formats, e.g. 5:00am\n",
    "            new_word = word.replace(\":\", \" \")\n",
    "            tokenised_text.append(new_word.lower())\n",
    "        else:\n",
    "            if (wn.morphy(word.lower()) != None):\n",
    "                tokenised_text.append(wn.morphy(word.lower()))\n",
    "            else:\n",
    "                tokenised_text.append(word.lower())\n",
    "    tokenised_text = list(filter(lambda x: x not in punctuation_and_extra, tokenised_text))\n",
    "    return tokenised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to read data from xml\n",
    "\n",
    "def xml2listData(xml_data):\n",
    "    data = []\n",
    "    root = ET.XML(xml_data)\n",
    "    for child in root:\n",
    "        instance = {}\n",
    "        for subchild in child:\n",
    "            if subchild.tag == 'text':\n",
    "                instance['text_id'] = child.attrib[\"id\"]\n",
    "                instance[subchild.tag] = preprocess_text(subchild.text)\n",
    "            if subchild.tag == 'questions':\n",
    "                questions = []\n",
    "                for question in subchild:\n",
    "                    single_question = {}\n",
    "                    single_question['question_id'] = question.attrib[\"id\"]\n",
    "                    single_question['question_text'] = preprocess_text(question.attrib[\"text\"])\n",
    "                    answers = []\n",
    "                    for answer in question:\n",
    "                        single_answer = {}\n",
    "                        single_answer['answer_id'] = answer.attrib[\"id\"]\n",
    "                        single_answer['answer_text'] = preprocess_text(answer.attrib[\"text\"])\n",
    "                        single_answer['correct'] = answer.attrib[\"correct\"]\n",
    "                        answers.append(single_answer)\n",
    "                    single_question['answers'] = answers\n",
    "                    questions.append(single_question)\n",
    "                instance['questions'] = questions\n",
    "        data.append(instance)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = xml2listData(xml_train)\n",
    "dev_data = xml2listData(xml_dev)\n",
    "test_data = xml2listData(xml_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing and preprocessing knowledge script data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to read DeScript xml\n",
    "\n",
    "def xml2listDeScript(xml_file):\n",
    "    data = []\n",
    "    root = ET.XML(xml_file)\n",
    "    for label in root:\n",
    "        for item in label:\n",
    "            data.append(preprocess_text(item.attrib[\"original\"]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "# returns a list of strings which are names of files\n",
    "first_gold_standard = glob.glob(\"./DeScript_LREC2016/gold_paraphrase_sets/first_gold_annotation/*.xml\")\n",
    "second_gold_standard = glob.glob(\"./DeScript_LREC2016/gold_paraphrase_sets/second_gold_annotation/*.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scriptFiles_to_lists(scriptFiles):\n",
    "    topcs = []\n",
    "    for filename in scriptFiles:\n",
    "        current_file = open(filename).read()\n",
    "        topcs.append(xml2listDeScript(current_file))\n",
    "    return topcs\n",
    "\n",
    "topcs1 = scriptFiles_to_lists(first_gold_standard)\n",
    "topcs2 = scriptFiles_to_lists(second_gold_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a dictionary with topics from both gold standards\n",
    "topics = {'baking_a_cake': np.concatenate(topcs1[0]+topcs2[0]),  'book_from_library': np.concatenate(topcs1[1]+topcs2[1]), 'airplane_flying': np.concatenate(topcs1[2]+topcs2[2]), \n",
    "          'hair_cut': np.concatenate(topcs1[3]+topcs2[3]), 'grocery_shopping': np.concatenate(topcs1[4]+topcs2[4]),'on_the_train': np.concatenate(topcs1[5]+topcs2[5]), \n",
    "          'planting_a_tree': np.concatenate(topcs1[6]+topcs2[6]), 'repair_flat_bike_tyre': np.concatenate(topcs1[7]+topcs2[7]), \n",
    "          'on_the_bus': np.concatenate(topcs1[8]+topcs2[8]), 'taking_bath': np.concatenate(topcs1[9]+topcs2[9])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a dictionary with docs within topics\n",
    "topic_docs = {'baking_a_cake': topcs1[0]+topcs2[0],  'book_from_library': topcs1[1]+topcs2[1], 'airplane_flying': topcs1[2]+topcs2[2], \n",
    "          'hair_cut': topcs1[3]+topcs2[3], 'grocery_shopping': topcs1[4]+topcs2[4],'on_the_train': topcs1[5]+topcs2[5], \n",
    "          'planting_a_tree': topcs1[6]+topcs2[6], 'repair_flat_bike_tyre': topcs1[7]+topcs2[7], \n",
    "          'on_the_bus': topcs1[8]+topcs2[8], 'taking_bath': topcs1[9]+topcs2[9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gold(data):\n",
    "    gold_standard = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            instance_result = [instance['text_id']]\n",
    "            instance_result.append(question['question_id'])\n",
    "            for answer in question['answers']:\n",
    "                if answer['correct'] == 'True':\n",
    "                    instance_result.append(answer['answer_id'])\n",
    "            gold_standard.append(instance_result)\n",
    "    return gold_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_train = get_gold(train_data)\n",
    "gold_dev = get_gold(dev_data)\n",
    "gold_test = get_gold(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluating accuracy, returning a tuple with percentage and list of incorrect answers\n",
    "def evaluate(results, ideal_results):\n",
    "    if (len(results) != len(ideal_results)):\n",
    "        raise Exception(\"Different length of your result and ideal results\")\n",
    "    correct_answers = 0\n",
    "    all_answers = len(results)\n",
    "    incorrect_answers = []\n",
    "    for i in range(all_answers):\n",
    "        if results[i] == ideal_results[i]:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            incorrect_answers.append(results[i])\n",
    "    return ((correct_answers/all_answers)*100, incorrect_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_results(results):\n",
    "    f = open('answer.txt', 'w')\n",
    "    for res in results:\n",
    "        f.write(\",\".join(res) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Questions\n",
    "- part 1: answer using only the given text\n",
    "- part 2: answer using other texts with similar topics\n",
    "- part 3: answer using text + knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Baseline. Answering Using The Given Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('917', '1', 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the longest answer length\n",
    "from operator import itemgetter\n",
    "all_answers = []\n",
    "for instance in train_data+dev_data:\n",
    "    for question in instance['questions']:\n",
    "        for answer in question['answers']:\n",
    "            all_answers.append((instance['text_id'], question['question_id'], len(answer['answer_text'])))\n",
    "max(all_answers,key=itemgetter(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_answer(text, answers):\n",
    "    answer_scores = {}\n",
    "    for answer in answers:\n",
    "        if \" \".join(answer['answer_text']) in \" \".join(text):\n",
    "            answer_scores[answer['answer_id']] = 28 #longest answer length\n",
    "        else:\n",
    "            clean_words = list(filter(lambda x: x not in stopwords, answer['answer_text']))\n",
    "            current_score = 0\n",
    "            for word in clean_words:\n",
    "                if word in text:\n",
    "                    current_score += 1\n",
    "            answer_scores[answer['answer_id']] = current_score \n",
    "    best_answer = max(answer_scores.items(), key=lambda k: k[1])\n",
    "    return best_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to answer yes-no questions\n",
    "def get_answer_for_general_form(question_text, text, answers):\n",
    "    correct_answer = 0\n",
    "    #leaving only meaningful words in the question\n",
    "    ques = list(filter(lambda x: x not in stopwords, question_text))\n",
    "    occurence_number = 0\n",
    "    for word in ques:\n",
    "        if word in text:\n",
    "            occurence_number += 1\n",
    "    if occurence_number > 2:\n",
    "        for answer in answers:\n",
    "            if 'yes' in answer['answer_text']:\n",
    "                return answer['answer_id']\n",
    "    else:\n",
    "        for answer in answers:\n",
    "            if 'yes' not in answer['answer_text']:\n",
    "                return answer['answer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_general_question(answers):\n",
    "    answer_texts = np.concatenate([answer['answer_text'] for answer in answers])\n",
    "    if \"yes\" in answer_texts:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#taking into account \"YES/NO\" questions\n",
    "# TODO: add doc2vec similarity between question & answer\n",
    "def choose_answer_v3(text, question_text, answers):\n",
    "    answer_scores = {}\n",
    "    #checking if the question is a general one\n",
    "    is_general = is_general_question(answers)\n",
    "    if is_general:\n",
    "        return get_answer_for_general_form(question_text, text, answers)\n",
    "    #looking for answer\n",
    "    for answer in answers:\n",
    "        if \" \".join(answer['answer_text']) in \" \".join(text):\n",
    "            answer_scores[answer['answer_id']] = 28 #longest answer length\n",
    "        else:\n",
    "            clean_words = list(filter(lambda x: x not in stopwords, answer['answer_text']))\n",
    "            current_score = 0\n",
    "            for word in clean_words:\n",
    "                if word in text:\n",
    "                    current_score += 1\n",
    "            answer_scores[answer['answer_id']] = current_score \n",
    "    best_answer = max(answer_scores.items(), key=lambda k: k[1])\n",
    "    return best_answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assuming data in the train/dev format (dictionaries of dictionaries and lists)\n",
    "# returning results in the form of [[0,1,1],[0,2,1]...]\n",
    "def run(data):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer(instance['text'], question['answers'])[0]\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v4(data, choose_answer_method):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer_method(instance['text'], question['question_text'],question['answers'])\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline train: 60.20963929709177\n",
      "Baseline dev: 60.45357902197024\n",
      "Baseline test: 60.707901322845906\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline train: \" + str(evaluate(run(train_data), gold_train)[0]))\n",
    "print(\"Baseline dev: \" + str(evaluate(run(dev_data), gold_dev)[0]))\n",
    "print(\"Baseline test: \" + str(evaluate(run(test_data), gold_test)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yes-No Distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes-No for train data: 58.976466961257834\n",
      "Yes-No for dev data: 56.909992912827775\n",
      "Yes-No for test data: 59.52806578476939\n"
     ]
    }
   ],
   "source": [
    "# Yes-No Distinction\n",
    "res_gen = run_v4(train_data, choose_answer_v3)\n",
    "print(\"Yes-No for train data: \" + str(evaluate(res_gen, gold_train)[0]))\n",
    "res_gen_dev = run_v4(dev_data, choose_answer_v3)\n",
    "print(\"Yes-No for dev data: \" + str(evaluate(res_gen_dev, gold_dev)[0]))\n",
    "res_gen_test = run_v4(test_data, choose_answer_v3)\n",
    "print(\"Yes-No for test data: \" + str(evaluate(res_gen_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing Incorrect Answers for Two Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20', '5', '1'],\n",
       " ['20', '6', '1'],\n",
       " ['20', '7', '1'],\n",
       " ['20', '8', '1'],\n",
       " ['21', '0', '1']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(res_gen, gold_train)[1][32:37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['19', '6', '0'],\n",
       " ['20', '1', '0'],\n",
       " ['20', '3', '1'],\n",
       " ['20', '5', '1'],\n",
       " ['20', '6', '1'],\n",
       " ['20', '7', '1']]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(run(train_data), gold_train)[1][33:39]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Answering Using Same-Cluster Texts\n",
    "- Modelling topics of train and dev data with BigARTM\n",
    "- Using texts from the same cluster to find the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modelling with BigARTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform into suitable input format and getting rid of common English words in the text to aid topic modelling\n",
    "f = open(\"all_texts.txt\", \"w\")\n",
    "all_texts = []\n",
    "for instance in train_data:\n",
    "    clean_text = instance['text']\n",
    "    clean_text = list(filter(lambda x: x not in stopwords, clean_text))\n",
    "    all_texts.append([\"train\" + instance['text_id'] + \" |text \" + \" \".join(clean_text)])\n",
    "for instance in dev_data:\n",
    "    clean_text = instance['text']\n",
    "    clean_text = list(filter(lambda x: x not in stopwords, clean_text))\n",
    "    all_texts.append([\"dev\" + instance['text_id'] + \" |text \" + \" \".join(clean_text)])\n",
    "for instance in test_data:\n",
    "    clean_text = instance['text']\n",
    "    clean_text = list(filter(lambda x: x not in stopwords, clean_text))\n",
    "    all_texts.append([\"test\" + instance['text_id'] + \" |text \" + \" \".join(clean_text)])\n",
    "\n",
    "for text in all_texts:\n",
    "    f.write(text[0]+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorise\n",
    "import artm\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path=\"all_texts.txt\", data_format=\"vowpal_wabbit\", target_folder=\"text_batches\")\n",
    "\n",
    "# create model\n",
    "T = 100 #number of topics\n",
    "model_artm = artm.ARTM(num_topics=110, topic_names=[\"topic\"+str(i) for i in range(110)], class_ids={\"text\":1}, \n",
    "                       num_document_passes=2, reuse_theta=True, cache_theta=True, seed=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise model\n",
    "dictionary = artm.Dictionary('dictionary')\n",
    "dictionary.gather(batch_vectorizer.data_path)\n",
    "\n",
    "model_artm.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore', class_id=\"text\"))\n",
    "model_artm.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore'))\n",
    "model_artm.scores.add(artm.TopTokensScore(name=\"top_words\", num_tokens=15, class_id=\"text\"))\n",
    "model_artm.scores.add(artm.PerplexityScore(name=\"PerplexityScore\", dictionary='dictionary'))\n",
    "\n",
    "model_artm.initialize('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_artm.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_artm.theta_columns_naming = \"title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = model_artm.get_theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic0\n",
      "['door', 'front', 'open', 'ring', 'hear', 'package', 'doorbell', 'say', 'man', 'could', 'close', 'know', 'back', 'answer', 'see']\n",
      "topic1\n",
      "['soup', 'pot', 'add', 'cook', 'make', 'chicken', 'rack', 'put', 'pan', 'dinner', 'dishwasher', 'ingredient', 'cooking', 'sauce', 'boil']\n",
      "topic2\n",
      "['plate', 'put', 'set', 'fork', 'table', 'one', 'napkin', 'place', 'silverware', 'cup', 'dinner', 'get', 'knife', 'eat', 'side']\n",
      "topic3\n",
      "['outside', 'get', 'work', 'sandwich', 'pick', 'tom', 'go', 'start', 'rain', 'container', 'job', 'would', 'going', 'put', 'home']\n",
      "topic4\n",
      "['would', 'go', 'though', 'going', 'could', 'well', 'water', 'sponge', 'jessie', 'back', 'like', 'love', 'dress', 'float', 'stay']\n",
      "topic5\n",
      "['dog', 'hot', 'put', 'cook', 'take', 'grill', 'mustard', 'bun', 'ketchup', 'buns', 'eat', 'make', 'microwave', 'place', 'cooking']\n",
      "topic6\n",
      "['lunch', 'time', 'know', 'late', 'go', 'quickly', 'home', 'around', 'would', 'today', 'sat', 'parent', 'heavy', 'help', 'order']\n",
      "topic7\n",
      "['dish', 'bread', 'toast', 'toaster', 'put', 'dry', 'butter', 'sink', 'slice', 'take', 'kitchen', 'one', 'pop', 'turn', 'make']\n",
      "topic8\n",
      "['gas', 'car', 'pump', 'tank', 'station', 'pull', 'fuel', 'back', 'nozzle', 'get', 'cap', 'put', 'fill', 'card', 'truck']\n",
      "topic9\n",
      "['room', 'living', 'start', 'one', 'bottom', 'top', 'next', 'small', 'like', 'little', 'dad', 'take', 'side', 'time', 'shelf']\n",
      "topic10\n",
      "['john', 'dad', 'newspaper', 'get', 'like', 'morning', 'wake', 'start', 'coffee', 'love', 'go', 'every', 'house', 'kind', 'want']\n",
      "topic11\n",
      "['wallpaper', 'cut', 'paper', 'would', 'get', 'apply', 'go', 'take', 'able', 'use', 'sweater', 'start', 'wound', 'read', 'design']\n",
      "topic12\n",
      "['say', 'mouth', 'go', 'dentist', 'teeth', 'chair', 'like', 'open', 'little', 'get', 'sat', 'take', 'drove', 'one', 'lady']\n",
      "topic13\n",
      "['camping', 'plastic', 'pack', 'set', 'tent', 'camp', 'bag', 'go', 'two', 'fire', 'put', 'get', 'sleeping', 'first', 'dinner']\n",
      "topic14\n",
      "['picnic', 'husband', 'decide', 'basket', 'get', 'nice', 'day', 'alex', 'blanket', 'park', 'going', 'fruit', 'potato', 'great', 'favorite']\n",
      "topic15\n",
      "['envelope', 'address', 'stamp', 'would', 'put', 'mailbox', 'seal', 'post', 'place', 'make', 'steve', 'name', 'office', 'take', 'could']\n",
      "topic16\n",
      "['doctor', 'check', 'go', 'blood', 'wait', 'one', 'bag', 'weight', 'take', 'pressure', 'pass', 'wound', 'boarding', 'nurse', 'part']\n",
      "topic17\n",
      "['know', 'cindy', 'going', 'take', 'time', 'things', 'like', 'line', 'lot', 'seats', 'ha', 'love', 'keep', 'along', 'around']\n",
      "topic18\n",
      "['us', 'order', 'come', 'waitress', 'go', 'back', 'two', 'husband', 'minutes', 'decide', 'ten', 'bring', 'wife', 'tell', 'take']\n",
      "topic19\n",
      "['pretty', 'piece', 'around', 'get', 'friend', 'need', 'fun', 'small', 'lot', 'right', 'weekend', 'usually', 'going', 'nice', 'start']\n",
      "topic20\n",
      "['painting', 'band', 'aid', 'hang', 'would', 'mark', 'get', 'finger', 'look', 'place', 'sure', 'make', 'put', 'cut', 'go']\n",
      "topic21\n",
      "['wash', 'put', 'bath', 'tub', 'laundry', 'water', 'clothes', 'dry', 'dirty', 'washing', 'load', 'take', 'bubble', 'finish', 'clean']\n",
      "topic22\n",
      "['pasta', 'minutes', 'sauce', 'add', 'let', 'put', 'get', 'cook', 'boil', 'heat', 'ready', 'need', 'make', 'also', 'first']\n",
      "topic23\n",
      "['table', 'mom', 'place', 'put', 'dinner', 'get', 'top', 'dishwasher', 'family', 'kitchen', 'glasses', 'plate', 'cup', 'dining', 'next']\n",
      "topic24\n",
      "['sponge', 'shower', 'scrub', 'sink', 'side', 'rinse', 'water', 'remove', 'band-aid', 'use', 'head', 'way', 'next', 'let', 'everything']\n",
      "topic25\n",
      "['night', 'get', 'tire', 'take', 'fall', 'last', 'little', 'asleep', 'new', 'week', 'sleep', 'back', 'bit', 'room', 'way']\n",
      "topic26\n",
      "['water', 'shower', 'get', 'soap', 'rinse', 'towel', 'body', 'turn', 'hot', 'take', 'dry', 'bucket', 'warm', 'start', 'clean']\n",
      "topic27\n",
      "['light', 'bulb', 'switch', 'back', 'one', 'turn', 'new', 'go', 'make', 'lamp', 'old', 'sure', 'screw', 'get', 'ladder']\n",
      "topic28\n",
      "['breakfast', 'eggs', 'bacon', 'morning', 'cook', 'make', 'toast', 'flip', 'wake', 'cheese', 'slice', 'butter', 'cooking', 'eat', 'place']\n",
      "topic29\n",
      "['beach', 'water', 'pack', 'go', 'towel', 'sand', 'get', 'bring', 'ocean', 'take', 'park', 'day', 'sun', 'could', 'umbrella']\n",
      "topic30\n",
      "['check', 'airport', 'plane', 'line', 'go', 'security', 'get', 'make', 'gate', 'flight', 'time', 'take', 'put', 'sure', 'pass']\n",
      "topic31\n",
      "['pay', 'bill', 'month', 'check', 'payment', 'money', 'one', 'company', 'account', 'get', 'credit', 'water', 'write', 'card', 'mail']\n",
      "topic32\n",
      "['flight', 'trip', 'go', 'ticket', 'want', 'plane', 'vacation', 'airport', 'fly', 'decide', 'airline', 'day', 'time', 'florida', 'need']\n",
      "topic33\n",
      "['pan', 'eggs', 'milk', 'egg', 'heat', 'stove', 'turn', 'make', 'add', 'pour', 'cook', 'get', 'omelette', 'put', 'bowl']\n",
      "topic34\n",
      "['water', 'pot', 'stove', 'place', 'pour', 'heat', 'turn', 'minutes', 'put', 'fill', 'boiling', 'pasta', 'boil', 'noodle', 'get']\n",
      "topic35\n",
      "['fish', 'food', 'tank', 'top', 'see', 'water', 'eat', 'small', 'feed', 'go', 'could', 'container', 'give', 'pond', 'take']\n",
      "topic36\n",
      "['garden', 'go', 'plant', 'work', 'want', 'grow', 'get', 'since', 'good', 'day', 'decide', 'one', 'make', 'place', 'finally']\n",
      "topic37\n",
      "['movie', 'watch', 'theater', 'go', 'dvd', 'want', 'decide', 'get', 'popcorn', 'rent', 'ticket', 'player', 'pick', 'tv', 'home']\n",
      "topic38\n",
      "['clothes', 'fold', 'put', 'clothing', 'sock', 'shirt', 'pants', 'laundry', 'folding', 'pair', 'pile', 'first', 'take', 'together', 'top']\n",
      "topic39\n",
      "['floor', 'clean', 'mop', 'cleaner', 'toilet', 'bathroom', 'use', 'sweep', 'cleaning', 'dust', 'wipe', 'get', 'first', 'make', 'around']\n",
      "topic40\n",
      "['bag', 'garbage', 'trash', 'take', 'open', 'top', 'pull', 'full', 'place', 'close', 'put', 'new', 'kitchen', 'grab', 'tie']\n",
      "topic41\n",
      "['orange', 'juice', 'bottle', 'fresh', 'juicer', 'get', 'make', 'drink', 'squeeze', 'put', 'half', 'formula', 'cut', 'go', 'hold']\n",
      "topic42\n",
      "['funeral', 'camera', 'take', 'photo', 'picture', 'family', 'make', 'photograph', 'sad', 'look', 'front', 'people', 'friend', 'stand', 'us']\n",
      "topic43\n",
      "['shoes', 'bowling', 'go', 'get', 'game', 'ball', 'alley', 'friend', 'lane', 'would', 'pin', 'us', 'size', 'one', 'put']\n",
      "topic44\n",
      "['sauna', 'steak', 'relax', 'go', 'back', 'bring', 'steam', 'hot', 'order', 'would', 'could', 'take', 'minutes', 'room', 'felt']\n",
      "topic45\n",
      "['play', 'tennis', 'ball', 'game', 'court', 'playing', 'hit', 'go', 'friend', 'back', 'get', 'point', 'time', 'decide', 'serve']\n",
      "topic46\n",
      "['baby', 'diaper', 'change', 'wipe', 'put', 'clean', 'take', 'back', 'get', 'would', 'sure', 'crying', 'need', 'make', 'go']\n",
      "topic47\n",
      "['book', 'read', 'found', 'library', 'go', 'reading', 'card', 'look', 'want', 'like', 'check', 'take', 'bring', 'get', 'desk']\n",
      "topic48\n",
      "['go', 'row', 'see', 'one', 'get', 'day', 'return', 'front', 'start', 'find', 'keep', 'decide', 'able', 'times', 'look']\n",
      "topic49\n",
      "['picture', 'take', 'online', 'look', 'want', 'get', 'information', 'could', 'go', 'like', 'price', 'see', 'decide', 'good', 'card']\n",
      "topic50\n",
      "['back', 'kid', 'inside', 'get', 'end', 'tie', 'bag', 'around', 'man', 'put', 'mom', 'well', 'outside', 'keep', 'exercise']\n",
      "topic51\n",
      "['coffee', 'filter', 'water', 'make', 'maker', 'cup', 'put', 'pour', 'grounds', 'machine', 'pot', 'take', 'morning', 'go', 'hot']\n",
      "topic52\n",
      "['trip', 'pack', 'suitcase', 'need', 'go', 'take', 'get', 'day', 'everything', 'bring', 'vacation', 'arrive', 'things', 'going', 'prepare']\n",
      "topic53\n",
      "['pool', 'towel', 'swimming', 'go', 'day', 'get', 'put', 'lightbulb', 'water', 'sunscreen', 'swim', 'walk', 'hot', 'suit', 'snack']\n",
      "topic54\n",
      "['food', 'cat', 'order', 'eat', 'get', 'restaurant', 'take', 'go', 'fast', 'place', 'give', 'feed', 'water', 'day', 'burger']\n",
      "topic55\n",
      "['hair', 'shampoo', 'conditioner', 'get', 'cut', 'wash', 'head', 'back', 'dry', 'make', 'haircut', 'rinse', 'time', 'chair', 'use']\n",
      "topic56\n",
      "['clean', 'cleaning', 'sink', 'put', 'bathroom', 'spray', 'back', 'dirty', 'go', 'take', 'brush', 'scrub', 'away', 'left', 'make']\n",
      "topic57\n",
      "['tree', 'paint', 'want', 'hole', 'would', 'get', 'large', 'newspaper', 'like', 'start', 'roller', 'fill', 'house', 'look', 'sure']\n",
      "topic58\n",
      "['vegetable', 'chop', 'cut', 'cutting', 'knife', 'onion', 'board', 'get', 'make', 'piece', 'put', 'carrot', 'salad', 'first', 'slice']\n",
      "topic59\n",
      "['felt', 'last', 'really', 'time', 'take', 'enjoy', 'get', 'would', 'experience', 'great', 'better', 'everything', 'good', 'want', 'start']\n",
      "topic60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cake', 'oven', 'mix', 'bake', 'ingredient', 'baking', 'pan', 'put', 'make', 'need', 'chocolate', 'get', 'frosting', 'recipe', 'bowl']\n",
      "topic61\n",
      "['wedding', 'bride', 'groom', 'ceremony', 'church', 'married', 'aisle', 'everyone', 'take', 'dress', 'couple', 'carrot', 'reception', 'exchange', 'walk']\n",
      "topic62\n",
      "['button', 'one', 'push', 'miss', 'like', 'find', 'hit', 'search', 'want', 'open', 'get', 'end', 'times', 'would', 'item']\n",
      "topic63\n",
      "['show', 'job', 'get', 'make', 'interview', 'way', 'downtown', 'beautiful', 'take', 'since', 'making', 'camera', 'schedule', 'decide', 'right']\n",
      "topic64\n",
      "['museum', 'go', 'exhibit', 'walk', 'could', 'dinosaur', 'saw', 'ticket', 'wait', 'see', 'room', 'old', 'brother', 'dad', 'mom']\n",
      "topic65\n",
      "['would', 'reservation', 'want', 'restaurant', 'make', 'ask', 'dinner', 'thought', 'could', 'time', 'available', 'return', 'like', 'decide', 'number']\n",
      "topic66\n",
      "['machine', 'vending', 'chips', 'snack', 'money', 'dollar', 'buy', 'get', 'candy', 'want', 'decide', 'store', 'put', 'found', 'go']\n",
      "topic67\n",
      "['bowl', 'things', 'cat', 'food', 'ha', 'like', 'back', 'store', 'fresh', 'go', 'time', 'pet', 'place', 'scoop', 'big']\n",
      "topic68\n",
      "['clean', 'use', 'shower', 'time', 'process', 'wall', 'cleaner', 'easy', 'put', 'dry', 'spray', 'first', 'one', 'scrub', 'cleaning']\n",
      "topic69\n",
      "['car', 'drive', 'turn', 'put', 'get', 'key', 'back', 'make', 'sure', 'drove', 'parking', 'road', 'right', 'park', 'side']\n",
      "topic70\n",
      "['walk', 'door', 'get', 'home', 'go', 'leash', 'around', 'open', 'back', 'walking', 'outside', 'front', 'start', 'saw', 'house']\n",
      "topic71\n",
      "['clothes', 'machine', 'laundry', 'put', 'washing', 'dryer', 'washer', 'basket', 'detergent', 'wash', 'dirty', 'start', 'take', 'dry', 'add']\n",
      "topic72\n",
      "['tea', 'water', 'cup', 'mug', 'put', 'kettle', 'bag', 'hot', 'make', 'take', 'drink', 'pour', 'teabag', 'minutes', 'get']\n",
      "topic73\n",
      "['fish', 'oil', 'put', 'piece', 'cook', 'good', 'pan', 'make', 'one', 'time', 'also', 'take', 'little', 'doe', 'buy']\n",
      "topic74\n",
      "['look', 'sit', 'nice', 'make', 'good', 'get', 'table', 'decide', 'one', 'sure', 'want', 'time', 'grab', 'set', 'put']\n",
      "topic75\n",
      "['tire', 'thread', 'button', 'needle', 'bike', 'shirt', 'back', 'tube', 'hole', 'put', 'air', 'one', 'sew', 'bicycle', 'use']\n",
      "topic76\n",
      "['train', 'station', 'ticket', 'get', 'take', 'york', 'new', 'walk', 'arrive', 'city', 'car', 'stop', 'seat', 'ride', 'come']\n",
      "topic77\n",
      "['barbecue', 'make', 'everyone', 'decide', 'people', 'well', 'choice', 'cup', 'day', 'like', 'start', 'meat', 'enjoy', 'cook', 'good']\n",
      "topic78\n",
      "['box', 'toy', 'paper', 'put', 'gift', 'tape', 'wrapping', 'wrap', 'fold', 'piece', 'side', 'make', 'top', 'one', 'around']\n",
      "topic79\n",
      "['salmon', 'get', 'go', 'work', 'minutes', 'back', 'underground', 'slow', 'crowd', 'let', 'subway', 'salad', 'pull', 'put', 'five']\n",
      "topic80\n",
      "['fire', 'wood', 'start', 'dry', 'go', 'get', 'small', 'piece', 'log', 'house', 'use', 'bonfire', 'could', 'stick', 'warm']\n",
      "topic81\n",
      "['paper', 'store', 'gift', 'buy', 'go', 'get', 'birthday', 'home', 'grocery', 'present', 'shop', 'wrap', 'purchase', 'need', 'wrapping']\n",
      "topic82\n",
      "['bed', 'sheet', 'pillow', 'put', 'blanket', 'make', 'top', 'take', 'corner', 'tuck', 'fit', 'place', 'mattress', 'pull', 'side']\n",
      "topic83\n",
      "['letter', 'write', 'envelope', 'mail', 'address', 'put', 'stamp', 'friend', 'pen', 'corner', 'life', 'writing', 'paper', 'top', 'right']\n",
      "topic84\n",
      "['driving', 'lesson', 'instructor', 'go', 'get', 'learn', 'take', 'nervous', 'car', 'park', 'start', 'really', 'driver', 'drive', 'time']\n",
      "topic85\n",
      "['mother', 'grandmother', 'billy', 'table', 'get', 'dinner', 'time', 'sister', 'come', 'together', 'flower', 'hold', 'ready', 'special', 'going']\n",
      "topic86\n",
      "['list', 'shopping', 'store', 'need', 'item', 'grocery', 'go', 'things', 'get', 'make', 'cart', 'week', 'put', 'check', 'buy']\n",
      "topic87\n",
      "['driver', 'taxi', 'bus', 'get', 'stop', 'take', 'minutes', 'street', 'cab', 'pay', 'arrive', 'wait', 'need', 'destination', 'people']\n",
      "topic88\n",
      "['pizza', 'order', 'minutes', 'decide', 'would', 'tell', 'want', 'give', 'delivery', 'phone', 'call', 'tip', 'get', 'ask', 'pay']\n",
      "topic89\n",
      "['book', 'library', 'card', 'librarian', 'borrow', 'need', 'back', 'ask', 'scan', 'research', 'would', 'looking', 'hand', 'tell', 'due']\n",
      "topic90\n",
      "['wall', 'room', 'new', 'paint', 'floor', 'look', 'decide', 'want', 'house', 'put', 'buy', 'color', 'get', 'painting', 'flooring']\n",
      "topic91\n",
      "['would', 'plane', 'get', 'seat', 'ticket', 'could', 'time', 'take', 'found', 'us', 'grill', 'window', 'day', 'hours', 'airport']\n",
      "topic92\n",
      "['friend', 'dance', 'go', 'drink', 'time', 'arrive', 'dancing', 'great', 'fun', 'decide', 'night', 'one', 'music', 'people', 'good']\n",
      "topic93\n",
      "['go', 'get', 'visit', 'house', 'talk', 'home', 'see', 'sister', 'time', 'family', 'cousin', 'going', 'around', 'excite', 'week']\n",
      "topic94\n",
      "['party', 'invitation', 'invite', 'birthday', 'friend', 'people', 'go', 'store', 'would', 'list', 'want', 'buy', 'envelope', 'get', 'decide']\n",
      "topic95\n",
      "['vacuum', 'carpet', 'closet', 'get', 'room', 'plug', 'back', 'sure', 'make', 'house', 'cord', 'furniture', 'move', 'turn', 'living']\n",
      "topic96\n",
      "['iron', 'ironing', 'shirt', 'board', 'pants', 'wrinkle', 'get', 'clothes', 'put', 'heat', 'take', 'set', 'steam', 'need', 'place']\n",
      "topic97\n",
      "['family', 'date', 'go', 'people', 'nice', 'would', 'person', 'like', 'see', 'time', 'take', 'one', 'know', 'good', 'drove']\n",
      "topic98\n",
      "['make', 'would', 'time', 'jenny', 'restaurant', 'day', 'next', 'want', 'weekend', 'decide', 'live', 'problem', 'call', 'one', 'way']\n",
      "topic99\n",
      "['lawn', 'mower', 'mow', 'grass', 'yard', 'start', 'gas', 'back', 'pull', 'put', 'go', 'turn', 'cut', 'push', 'shed']\n",
      "topic100\n",
      "['room', 'appointment', 'call', 'dentist', 'doctor', 'office', 'come', 'tell', 'teeth', 'name', 'receptionist', 'go', 'back', 'check', 'make']\n",
      "topic101\n",
      "['medicine', 'child', 'take', 'go', 'bottle', 'get', 'cup', 'need', 'put', 'make', 'back', 'water', 'feel', 'sleep', 'give']\n",
      "topic102\n",
      "['phone', 'call', 'tell', 'say', 'ask', 'answer', '911', 'help', 'talk', 'number', 'could', 'hear', 'house', 'police', 'cell']\n",
      "topic103\n",
      "['battery', 'clock', 'alarm', 'back', 'time', 'go', 'work', 'one', 'take', 'store', 'morning', 'set', 'change', 'two', 'old']\n",
      "topic104\n",
      "['bonfire', 'make', 'fire', 'start', 'around', 'gather', 'burn', 'friend', 'rock', 'catch', 'go', 'pit', 'would', 'place', 'lighter']\n",
      "topic105\n",
      "['bus', 'get', 'ride', 'take', 'stop', 'always', 'work', 'time', 'need', 'school', 'near', 'public', 'would', 'walk', 'driver']\n",
      "topic106\n",
      "['plant', 'tree', 'hole', 'water', 'garden', 'seed', 'would', 'soil', 'roots', 'make', 'dirt', 'grow', 'back', 'dug', 'place']\n",
      "topic107\n",
      "['tell', 'ask', 'say', 'take', 'get', 'minutes', 'come', 'make', 'would', 'drove', 'time', 'left', 'wait', 'go', 'want']\n",
      "topic108\n",
      "['make', 'sure', 'would', 'also', 'enough', 'go', 'big', 'come', 'mom', 'pick', 'want', 'days', 'brother', 'put', 'different']\n",
      "topic109\n",
      "['stove', 'pasta', 'boil', 'turn', 'heat', 'flame', 'boiling', 'start', 'pot', 'meat', 'bubble', 'cook', 'meal', 'first', 'want']\n"
     ]
    }
   ],
   "source": [
    "# top words for each topic\n",
    "for topic in model_artm.score_tracker[\"top_words\"].last_tokens:\n",
    "    print(topic)\n",
    "    print(model_artm.score_tracker[\"top_words\"].last_tokens[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model_artm.save(\"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic33</th>\n",
       "      <td>0.022541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic90</th>\n",
       "      <td>0.020207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic26</th>\n",
       "      <td>0.020067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic82</th>\n",
       "      <td>0.016940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic37</th>\n",
       "      <td>0.016566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic39</th>\n",
       "      <td>0.015212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>0.015137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic102</th>\n",
       "      <td>0.014618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic71</th>\n",
       "      <td>0.014081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic83</th>\n",
       "      <td>0.013499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic54</th>\n",
       "      <td>0.013263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic86</th>\n",
       "      <td>0.013113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic55</th>\n",
       "      <td>0.013035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic69</th>\n",
       "      <td>0.012581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic70</th>\n",
       "      <td>0.012306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic93</th>\n",
       "      <td>0.012295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic32</th>\n",
       "      <td>0.012219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic92</th>\n",
       "      <td>0.012175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic23</th>\n",
       "      <td>0.011674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic75</th>\n",
       "      <td>0.011569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic21</th>\n",
       "      <td>0.011393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic100</th>\n",
       "      <td>0.011324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic76</th>\n",
       "      <td>0.011138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic38</th>\n",
       "      <td>0.011104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic87</th>\n",
       "      <td>0.010896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic78</th>\n",
       "      <td>0.010724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic65</th>\n",
       "      <td>0.010703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic94</th>\n",
       "      <td>0.010635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic40</th>\n",
       "      <td>0.010562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic46</th>\n",
       "      <td>0.010520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic9</th>\n",
       "      <td>0.006617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic10</th>\n",
       "      <td>0.006603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic13</th>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic73</th>\n",
       "      <td>0.006225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic25</th>\n",
       "      <td>0.006197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic48</th>\n",
       "      <td>0.006187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic12</th>\n",
       "      <td>0.006157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic104</th>\n",
       "      <td>0.006150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic67</th>\n",
       "      <td>0.006119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic89</th>\n",
       "      <td>0.006104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic57</th>\n",
       "      <td>0.005863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic80</th>\n",
       "      <td>0.005818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic63</th>\n",
       "      <td>0.005801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic77</th>\n",
       "      <td>0.005563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic98</th>\n",
       "      <td>0.005523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic14</th>\n",
       "      <td>0.005507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic11</th>\n",
       "      <td>0.005453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic91</th>\n",
       "      <td>0.005396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic36</th>\n",
       "      <td>0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic61</th>\n",
       "      <td>0.005333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic16</th>\n",
       "      <td>0.005004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic85</th>\n",
       "      <td>0.004963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic15</th>\n",
       "      <td>0.004853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic6</th>\n",
       "      <td>0.004777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic79</th>\n",
       "      <td>0.004523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic3</th>\n",
       "      <td>0.004503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic4</th>\n",
       "      <td>0.004216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic50</th>\n",
       "      <td>0.004185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic19</th>\n",
       "      <td>0.004081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic17</th>\n",
       "      <td>0.003625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Probability\n",
       "topic33      0.022541\n",
       "topic90      0.020207\n",
       "topic26      0.020067\n",
       "topic82      0.016940\n",
       "topic37      0.016566\n",
       "topic39      0.015212\n",
       "topic1       0.015137\n",
       "topic102     0.014618\n",
       "topic71      0.014081\n",
       "topic83      0.013499\n",
       "topic54      0.013263\n",
       "topic86      0.013113\n",
       "topic55      0.013035\n",
       "topic69      0.012581\n",
       "topic70      0.012306\n",
       "topic93      0.012295\n",
       "topic32      0.012219\n",
       "topic92      0.012175\n",
       "topic23      0.011674\n",
       "topic75      0.011569\n",
       "topic21      0.011393\n",
       "topic100     0.011324\n",
       "topic76      0.011138\n",
       "topic38      0.011104\n",
       "topic87      0.010896\n",
       "topic78      0.010724\n",
       "topic65      0.010703\n",
       "topic94      0.010635\n",
       "topic40      0.010562\n",
       "topic46      0.010520\n",
       "...               ...\n",
       "topic9       0.006617\n",
       "topic10      0.006603\n",
       "topic13      0.006228\n",
       "topic73      0.006225\n",
       "topic25      0.006197\n",
       "topic48      0.006187\n",
       "topic12      0.006157\n",
       "topic104     0.006150\n",
       "topic67      0.006119\n",
       "topic89      0.006104\n",
       "topic57      0.005863\n",
       "topic80      0.005818\n",
       "topic63      0.005801\n",
       "topic77      0.005563\n",
       "topic98      0.005523\n",
       "topic14      0.005507\n",
       "topic11      0.005453\n",
       "topic91      0.005396\n",
       "topic36      0.005346\n",
       "topic61      0.005333\n",
       "topic16      0.005004\n",
       "topic85      0.004963\n",
       "topic15      0.004853\n",
       "topic6       0.004777\n",
       "topic79      0.004523\n",
       "topic3       0.004503\n",
       "topic4       0.004216\n",
       "topic50      0.004185\n",
       "topic19      0.004081\n",
       "topic17      0.003625\n",
       "\n",
       "[110 rows x 1 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_probabilities = []\n",
    "for i, topic in enumerate(model_artm.topic_names):\n",
    "    topic_probabilities.append(sum(theta.as_matrix()[i])/len(theta.as_matrix()[i]))\n",
    "\n",
    "probability_df = pd.DataFrame(columns=[\"Probability\"], index=model_artm.topic_names, data=topic_probabilities)\n",
    "probability_df.sort_values(\"Probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that __topic4__ has the biggest probability across all documents, especially compared to all others: around 69%. From top tokens, contained in this topic, we see that it's a topic with commonly used verbs and other parts of speech. Other similar topics include __topic0__ and __topic10__. Therefore, I can try to take the next best possibility for each document, disregarding the one for these three topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_by_id(text_id, data):\n",
    "    text = {}\n",
    "    for instance in data:\n",
    "        if instance['text_id'] == text_id:\n",
    "            text['text_id'] = text_id\n",
    "            text['text'] = instance['text']\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mapping to separate data-s\n",
    "train_topic2docs = {}\n",
    "dev_topic2docs = {}\n",
    "test_topic2docs = {}\n",
    "for col in theta.columns:\n",
    "    max_proba = max(theta.loc[:,col])\n",
    "    best_topic = theta.loc[theta[col] == max_proba].index\n",
    "    if 'train' in col:\n",
    "        current_text = get_text_by_id(col[5:], train_data)\n",
    "        if best_topic[0] not in train_topic2docs:\n",
    "            train_topic2docs[best_topic[0]] = []\n",
    "            train_topic2docs[best_topic[0]].append(current_text)\n",
    "        else:\n",
    "            train_topic2docs[best_topic[0]].append(current_text)\n",
    "    elif 'dev' in col:\n",
    "        current_text = get_text_by_id(col[3:], dev_data)\n",
    "        if best_topic[0] not in dev_topic2docs:\n",
    "            dev_topic2docs[best_topic[0]] = []\n",
    "            dev_topic2docs[best_topic[0]].append(current_text)\n",
    "        else:\n",
    "            dev_topic2docs[best_topic[0]].append(current_text)\n",
    "    elif 'test' in col:\n",
    "        current_text = get_text_by_id(col[4:], test_data)\n",
    "        if best_topic[0] not in test_topic2docs:\n",
    "            test_topic2docs[best_topic[0]] = []\n",
    "            test_topic2docs[best_topic[0]].append(current_text)\n",
    "        else:\n",
    "            test_topic2docs[best_topic[0]].append(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mapping from topics to texts\n",
    "# {topic: [{text_id:text, ...}], topic: ...}\n",
    "topic2docs = {}\n",
    "for col in theta.columns:\n",
    "    max_proba = max(theta.loc[:,col])\n",
    "    best_topic = theta.loc[theta[col] == max_proba].index\n",
    "    if best_topic[0] not in topic2docs:\n",
    "        topic2docs[best_topic[0]] = []\n",
    "        topic2docs[best_topic[0]].append(col)\n",
    "    else:\n",
    "        topic2docs[best_topic[0]].append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+18WzwVk6NfuhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1124a6518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = [len(value) for value in topic2docs.values()]\n",
    "plt.barh([i for i in range(len(topic2docs.keys()))], lens)\n",
    "plt.savefig(\"fig_0.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the new distribution for document per topic. __Topic7__ is prevalent but it's not as bad as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 106 artists>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwvx/NBitsWg3138AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bc36a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = [len(value) for value in test_topic2docs.values()]\n",
    "keys = len(test_topic2docs.keys())\n",
    "plt.barh([i for i in range(keys)], lens, tick_label = test_topic2docs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing an answer given texts from the same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a method to choose answer from the original text AND from texts from the same cluster\n",
    "# answer as dictionary, original text as list of strings, similar_texts as one list of strings\n",
    "def choose_answer_v2(original_text, similar_texts, answers):\n",
    "    original_answer = choose_answer(original_text, answers)\n",
    "    #if we don't find the answer in the original text\n",
    "    if original_answer[1] == 0:\n",
    "        similar_answer = choose_answer(similar_texts, answers)\n",
    "        return similar_answer\n",
    "    else:\n",
    "        return original_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_by_text_id(text_id, topic2docs):\n",
    "    for topic in topic2docs.keys():\n",
    "        for text in topic2docs[topic]:\n",
    "            if text['text_id'] == text_id:\n",
    "                return topic\n",
    "\n",
    "def get_topic_texts(topic, topic2docs):\n",
    "    texts_list = topic2docs[topic]\n",
    "    texts = []\n",
    "    for text in texts_list:\n",
    "        texts += text['text']\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v2(data, topic2docs, choose_answer_function):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        instance_topic = get_topic_by_text_id(instance['text_id'], topic2docs)\n",
    "        similar_texts = get_topic_texts(instance_topic, topic2docs)\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer_function(instance['text'], similar_texts, question['answers'])[0]\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigARTM train accuracy: 61.5044702497174\n",
      "BigARTM dev accuracy: 61.09142452161588\n",
      "BigARTM test accuracy: 61.38720057204148\n"
     ]
    }
   ],
   "source": [
    "train_with_bigARTM = run_v2(train_data, train_topic2docs, choose_answer_v2)\n",
    "print(\"BigARTM train accuracy: \" + str(evaluate(train_with_bigARTM, gold_train)[0]))\n",
    "dev_with_bigARTM = run_v2(dev_data, dev_topic2docs, choose_answer_v2)\n",
    "print(\"BigARTM dev accuracy: \" + str(evaluate(dev_with_bigARTM, gold_dev)[0]))\n",
    "test_with_bigARTM = run_v2(test_data, test_topic2docs, choose_answer_v2)\n",
    "print(\"BigARTM test accuracy: \" + str(evaluate(test_with_bigARTM, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 incorrent answers: comparison with baseline\n",
      "[['7', '5', '1'], ['8', '2', '0'], ['8', '4', '0'], ['9', '0', '0'], ['10', '2', '0'], ['10', '3', '0'], ['10', '5', '0'], ['10', '7', '0'], ['11', '0', '0'], ['11', '1', '0']]\n",
      "[['8', '4', '0'], ['10', '2', '0'], ['10', '3', '0'], ['10', '5', '0'], ['10', '7', '0'], ['10', '8', '1'], ['11', '0', '0'], ['11', '1', '0'], ['11', '3', '0'], ['11', '4', '1']]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 incorrent answers: comparison with baseline\")\n",
    "print(evaluate(run(test_data), gold_test)[1][30:40])\n",
    "print(evaluate(test_with_bigARTM, gold_test)[1][30:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Answering Using Related Script Knowledge\n",
    "terminology: _topic-script_ is a topic section from DeScript, e.g. baking a cake, flying in an airplane\n",
    "- mapping text to topic-script\n",
    "- finding answer in the topic-script\n",
    "\n",
    "_IDEA_: doc similarity between topic-script and given text with Doc2Vec<br>\n",
    "_IDEA_: map text to a specific event from the scripts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Similarity Between Given Text and Topic-Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper method - from tuples to dictionary\n",
    "def tuple_to_dict(some_list):\n",
    "    new_list = {}\n",
    "    for pair in some_list:\n",
    "        new_list[pair[0]] = pair[1]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding frequencies for topics and scaling them by number of documents\n",
    "topic_vectors = {}\n",
    "for topic in topics.keys():\n",
    "    topic_vectors[topic] = {}\n",
    "    meaningful_words = list(filter(lambda x: x not in stopwords, topics[topic]))\n",
    "    words = tuple_to_dict(nltk.FreqDist(meaningful_words).most_common(12))\n",
    "    \n",
    "    current_docs = topic_docs[topic]\n",
    "    for word in words:\n",
    "        number_of_docs = 0\n",
    "        for doc in current_docs:\n",
    "            if word in doc:\n",
    "                number_of_docs += 1\n",
    "        topic_vectors[topic][word] = words[word]/number_of_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting all texts and cleaning\n",
    "clean_all_texts = []\n",
    "for text in all_texts:\n",
    "    ind = text[0].index(\"|\")\n",
    "    clean_all_texts.append([text[0][ind+6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting frequencies for words in all documents\n",
    "full_texts = []\n",
    "full_texts = np.append(full_texts, clean_all_texts)\n",
    "just_text = \" \".join(full_texts)\n",
    "words = nltk.tokenize.word_tokenize(just_text)\n",
    "fd = nltk.FreqDist(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleaning and finding frequencies for given text\n",
    "def get_text_vector(text):\n",
    "    meaningful_words = list(filter(lambda x: x not in stopwords, text))\n",
    "    text_vector = tuple_to_dict(nltk.FreqDist(meaningful_words).most_common(12))\n",
    "    for word, freq in text_vector.items():\n",
    "        if word in fd:\n",
    "            text_vector[word] = freq/(fd[word])\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# cosine similarity function to compare two vectors with frequencies\n",
    "def cosine_similarity(topic_vector, text_vector):\n",
    "    \n",
    "    topic_sum = 0\n",
    "    text_sum = 0\n",
    "    dot_product = 0\n",
    "    \n",
    "    for word in topic_vector:\n",
    "        topic_sum += (topic_vector[word]**2)\n",
    "        if word in text_vector.keys():\n",
    "            text_sum += (text_vector[word]**2)\n",
    "            dot_product += (topic_vector[word] * text_vector[word])\n",
    "    topic_norm = math.sqrt(topic_sum)\n",
    "    text_norm = math.sqrt(text_sum)\n",
    "    \n",
    "    if topic_norm * text_norm == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product/(topic_norm * text_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns most similar topic\n",
    "def get_topic(text, topic_vectors):\n",
    "    topic_scores = {}\n",
    "    for topic in topic_vectors.keys():\n",
    "        csim = cosine_similarity(topic_vectors[topic], get_text_vector(text))\n",
    "        topic_scores[topic] = csim\n",
    "    return max(topic_scores.items(), key=lambda k: k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering Using Topic-Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v3(data, topic_vectors, topics, choose_answer_function):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            current_topic = get_topic(instance['text'], topic_vectors)[0]\n",
    "            correct_answer = choose_answer_function(instance['text'], topics[current_topic], question['answers'])[0]\n",
    "            results.append([instance['text_id'], question['question_id'], correct_answer])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descript accuracy train: 60.41516801973076\n",
      "Descript accuracy dev: 59.67399007795889\n",
      "Descript accuracy test: 61.67322130854487\n"
     ]
    }
   ],
   "source": [
    "des_train = run_v3(train_data, topic_vectors, topics, choose_answer_v2)\n",
    "print(\"Descript accuracy train: \" + str(evaluate(des_train, gold_train)[0]))\n",
    "des_dev = run_v3(dev_data, topic_vectors, topics, choose_answer_v2)\n",
    "print(\"Descript accuracy dev: \" + str(evaluate(des_dev, gold_dev)[0]))\n",
    "des_test = run_v3(test_data, topic_vectors, topics, choose_answer_v2)\n",
    "print(\"Descript accuracy test: \" + str(evaluate(des_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Clustering with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- part 1: embed texts with Doc2Vec\n",
    "- part 2: cluster embedded representations with KMeans (or other clustering methods)\n",
    "- part 3: cluster - texts mapping\n",
    "- part 4: choose answer, using a given text and texts from the same cluster\n",
    "\n",
    "_Inspired by_: https://towardsdatascience.com/automatic-topic-clustering-using-doc2vec-e1cea88449c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Doc2Vec and Inferring Vectors for all Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for t in clean_all_texts:\n",
    "    data.append(nltk.tokenize.word_tokenize(t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additional_texts_from_descript = []\n",
    "for topic in topics:\n",
    "    l = len(topics[topic])\n",
    "    data.append(topics[topic][:int(l/3)])\n",
    "    data.append(topics[topic][int(l/3):int(l-(l/3))])\n",
    "    data.append(topics[topic][int(l-(l/3)):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Documents(object):\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            yield TaggedDocument(words = doc, tags = [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = Documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "epoch 20\n",
      "epoch 21\n",
      "epoch 22\n",
      "epoch 23\n",
      "epoch 24\n",
      "epoch 25\n",
      "epoch 26\n",
      "epoch 27\n",
      "epoch 28\n",
      "epoch 29\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "count = len(data)\n",
    "model = Doc2Vec(size=100, dbow_words= 1, dm=0, iter=1,  window=12, seed=1337, min_count=5, workers=4,alpha=0.025, min_alpha=0.025)\n",
    "model.build_vocab(documents)\n",
    "for epoch in range(30):\n",
    "    print(\"epoch \"+str(epoch))\n",
    "    model.train(documents, total_examples=count, epochs=1)\n",
    "    model.save('commonSense.model')\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inferring vectors\n",
    "vectors = []\n",
    "for d in data:\n",
    "    vectors.append(model.infer_vector(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclusterer = KMeansClusterer(T, distance=nltk.cluster.util.cosine_distance, repeats=20)\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111cf7cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of clusters per all 2119 texts (from train, dev and test combined)\n",
    "cnt = Counter(assigned_clusters)\n",
    "plt.barh([i for i in range(len(cnt.keys()))], [value for value in cnt.values()])\n",
    "plt.savefig(\"fig_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text2Cluster/Cluster2Texts Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from assigned clusters to actual text_ids from all_texts\n",
    "mapping_text2cluster = {}\n",
    "for i in range(len(all_texts)):\n",
    "    cur_ind = all_texts[i][0].index(\"|\")\n",
    "    text_id = all_texts[i][0][:cur_ind-1]\n",
    "    mapping_text2cluster[text_id] = assigned_clusters[i]\n",
    "mapping_text2cluster['test429']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapping from cluster number to text_ids\n",
    "mapping_cluster2texts = {}\n",
    "for key,value in list(mapping_text2cluster.items()):\n",
    "    if value not in mapping_cluster2texts:\n",
    "        mapping_cluster2texts[value] = []\n",
    "        mapping_cluster2texts[value].append(key)\n",
    "    else:\n",
    "        mapping_cluster2texts[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(28, 39), (51, 40), (87, 58)]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most frequent clusters\n",
    "map_cluster_len = {}\n",
    "for key, value in mapping_cluster2texts.items():\n",
    "    map_cluster_len[key] = len(value)\n",
    "\n",
    "import operator\n",
    "sorted_len = sorted(map_cluster_len.items(), key=operator.itemgetter(1))\n",
    "sorted_len[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_from_mapping_id(mapping_id):\n",
    "    if 'train' in mapping_id:\n",
    "        text_id = mapping_id[5:]\n",
    "        return get_text_by_id(text_id, train_data)['text']\n",
    "    elif 'dev' in mapping_id:\n",
    "        text_id = mapping_id[3:]\n",
    "        return get_text_by_id(text_id, dev_data)['text']\n",
    "    elif 'test' in mapping_id:\n",
    "        text_id = mapping_id[4:]\n",
    "        return get_text_by_id(text_id, test_data)['text']\n",
    "    \n",
    "# returns list of lists (with texts divided into words)\n",
    "def get_all_texts_per_cluster(cluster_number, mapping_cluster2texts):\n",
    "    current_mapping_ids = mapping_cluster2texts[cluster_number]\n",
    "    texts_per_cluster = []\n",
    "    for map_id in current_mapping_ids:\n",
    "        texts_per_cluster.append(get_text_from_mapping_id(map_id))\n",
    "    return texts_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODIFIED TO GET CLEAN TEXTS\n",
    "def get_text_from_mapping_id_clean(mapping_id):\n",
    "    if 'train' in mapping_id:\n",
    "        text_id = mapping_id[5:]\n",
    "        text_train = get_text_by_id(text_id, train_data)['text']\n",
    "        return list(filter(lambda x: x not in stopwords, text_train))\n",
    "    elif 'dev' in mapping_id:\n",
    "        text_id = mapping_id[3:]\n",
    "        text_dev = get_text_by_id(text_id, dev_data)['text']\n",
    "        return list(filter(lambda x: x not in stopwords, text_dev))\n",
    "    elif 'test' in mapping_id:\n",
    "        text_id = mapping_id[4:]\n",
    "        text_test = get_text_by_id(text_id, test_data)['text']\n",
    "        return list(filter(lambda x: x not in stopwords, text_test))\n",
    "    \n",
    "# returns list of lists (with texts divided into words)\n",
    "def get_all_texts_per_cluster(cluster_number, mapping_cluster2texts):\n",
    "    current_mapping_ids = mapping_cluster2texts[cluster_number]\n",
    "    texts_per_cluster = []\n",
    "    for map_id in current_mapping_ids:\n",
    "        texts_per_cluster.append(get_text_from_mapping_id_clean(map_id))\n",
    "    return texts_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 6: [('movie', 112), ('dvd', 50), ('watch', 46), ('player', 32), ('play', 23), ('get', 20), ('want', 19), ('decide', 19), ('friend', 18), ('go', 17), ('turn', 17), ('tv', 16), ('back', 14), ('one', 12), ('night', 12)]\n",
      "cluster 38: [('sauna', 19), ('go', 18), ('room', 13), ('get', 10), ('towel', 9), ('relax', 7), ('minutes', 7), ('change', 6), ('back', 6), ('clothes', 5), ('left', 5), ('use', 5), ('us', 5), ('pay', 4), ('take', 4)]\n",
      "cluster 75: [('go', 49), ('store', 40), ('get', 30), ('buy', 22), ('shopping', 20), ('car', 20), ('clothes', 18), ('shoes', 18), ('new', 17), ('decide', 16), ('back', 15), ('friend', 15), ('walk', 14), ('like', 14), ('dress', 13)]\n"
     ]
    }
   ],
   "source": [
    "# to find out top 15 tokens for 3 most frequent clusters\n",
    "print(\"cluster 6: \" + str(nltk.FreqDist(np.concatenate(get_all_texts_per_cluster(6, mapping_cluster2texts))).most_common(15)))\n",
    "print(\"cluster 38: \"  + str(nltk.FreqDist(np.concatenate(get_all_texts_per_cluster(38, mapping_cluster2texts))).most_common(15)))\n",
    "print(\"cluster 75: \" + str(nltk.FreqDist(np.concatenate(get_all_texts_per_cluster(75, mapping_cluster2texts))).most_common(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing an Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v5(data, mapping_cluster2texts, mapping_text2cluster, datatype_prefix):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        cluster_number = mapping_text2cluster[datatype_prefix + instance['text_id']]\n",
    "        similar_texts = np.concatenate(get_all_texts_per_cluster(cluster_number, mapping_cluster2texts))\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer_v2(instance['text'], similar_texts, question['answers'])[0]\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 61.06258349604358\n",
      "Dev accuracy: 60.09922041105599\n",
      "Test accuracy: 61.70897390060779\n"
     ]
    }
   ],
   "source": [
    "# 20 epochs, no DeScript, window 10\n",
    "gensim_1_train = run_v5(train_data, mapping_cluster2texts, mapping_text2cluster, 'train')\n",
    "print(\"Train accuracy: \" + str(evaluate(gensim_1_train, gold_train)[0]))\n",
    "gensim_1_dev = run_v5(dev_data, mapping_cluster2texts, mapping_text2cluster, 'dev')\n",
    "print(\"Dev accuracy: \" + str(evaluate(gensim_1_dev, gold_dev)[0]))\n",
    "gensim_1_test = run_v5(test_data, mapping_cluster2texts, mapping_text2cluster, 'test')\n",
    "print(\"Test accuracy: \" + str(evaluate(gensim_1_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 61.689446100092496\n",
      "Dev accuracy: 61.23316796598157\n",
      "Test accuracy: 61.60171612441903\n"
     ]
    }
   ],
   "source": [
    "# 20 epochs, no DeScript, window 12\n",
    "gensim_1_train = run_v5(train_data, mapping_cluster2texts, mapping_text2cluster, 'train')\n",
    "print(\"Train accuracy: \" + str(evaluate(gensim_1_train, gold_train)[0]))\n",
    "gensim_1_dev = run_v5(dev_data, mapping_cluster2texts, mapping_text2cluster, 'dev')\n",
    "print(\"Dev accuracy: \" + str(evaluate(gensim_1_dev, gold_dev)[0]))\n",
    "gensim_1_test = run_v5(test_data, mapping_cluster2texts, mapping_text2cluster, 'test')\n",
    "print(\"Test accuracy: \" + str(evaluate(gensim_1_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 61.59695817490495\n",
      "Dev accuracy: 61.09142452161588\n",
      "Test accuracy: 61.56596353235609\n"
     ]
    }
   ],
   "source": [
    "# 20 epochs, DeScript, window 10\n",
    "gensim_1_train = run_v5(train_data, mapping_cluster2texts, mapping_text2cluster, 'train')\n",
    "print(\"Train accuracy: \" + str(evaluate(gensim_1_train, gold_train)[0]))\n",
    "gensim_1_dev = run_v5(dev_data, mapping_cluster2texts, mapping_text2cluster, 'dev')\n",
    "print(\"Dev accuracy: \" + str(evaluate(gensim_1_dev, gold_dev)[0]))\n",
    "gensim_1_test = run_v5(test_data, mapping_cluster2texts, mapping_text2cluster, 'test')\n",
    "print(\"Test accuracy: \" + str(evaluate(gensim_1_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 61.60723461103689\n",
      "Dev accuracy: 60.737065910701624\n",
      "Test accuracy: 61.74472649267072\n"
     ]
    }
   ],
   "source": [
    "# 30 epochs, no DeScript, window 10\n",
    "gensim_1_train = run_v5(train_data, mapping_cluster2texts, mapping_text2cluster, 'train')\n",
    "print(\"Train accuracy: \" + str(evaluate(gensim_1_train, gold_train)[0]))\n",
    "gensim_1_dev = run_v5(dev_data, mapping_cluster2texts, mapping_text2cluster, 'dev')\n",
    "print(\"Dev accuracy: \" + str(evaluate(gensim_1_dev, gold_dev)[0]))\n",
    "gensim_1_test = run_v5(test_data, mapping_cluster2texts, mapping_text2cluster, 'test')\n",
    "print(\"Test accuracy: \" + str(evaluate(gensim_1_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 61.61751104716884\n",
      "Dev accuracy: 61.02055279943303\n",
      "Test accuracy: 61.887736860922416\n"
     ]
    }
   ],
   "source": [
    "# 30 epochs, no DeScript, window 12\n",
    "gensim_1_train = run_v5(train_data, mapping_cluster2texts, mapping_text2cluster, 'train')\n",
    "print(\"Train accuracy: \" + str(evaluate(gensim_1_train, gold_train)[0]))\n",
    "gensim_1_dev = run_v5(dev_data, mapping_cluster2texts, mapping_text2cluster, 'dev')\n",
    "print(\"Dev accuracy: \" + str(evaluate(gensim_1_dev, gold_dev)[0]))\n",
    "gensim_1_test = run_v5(test_data, mapping_cluster2texts, mapping_text2cluster, 'test')\n",
    "print(\"Test accuracy: \" + str(evaluate(gensim_1_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 61.494193813585454\n",
      "Dev accuracy: 60.66619418851879\n",
      "Test accuracy: 61.24419020378977\n"
     ]
    }
   ],
   "source": [
    "# 30 epochs, DeScript, window 10\n",
    "gensim_1_train = run_v5(train_data, mapping_cluster2texts, mapping_text2cluster, 'train')\n",
    "print(\"Train accuracy: \" + str(evaluate(gensim_1_train, gold_train)[0]))\n",
    "gensim_1_dev = run_v5(dev_data, mapping_cluster2texts, mapping_text2cluster, 'dev')\n",
    "print(\"Dev accuracy: \" + str(evaluate(gensim_1_dev, gold_dev)[0]))\n",
    "gensim_1_test = run_v5(test_data, mapping_cluster2texts, mapping_text2cluster, 'test')\n",
    "print(\"Test accuracy: \" + str(evaluate(gensim_1_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 61.59695817490495\n",
      "Dev accuracy: 60.24096385542169\n",
      "Test accuracy: 61.95924204504827\n"
     ]
    }
   ],
   "source": [
    "# 30 epochs, DeScript, window 12\n",
    "gensim_1_train = run_v5(train_data, mapping_cluster2texts, mapping_text2cluster, 'train')\n",
    "print(\"Train accuracy: \" + str(evaluate(gensim_1_train, gold_train)[0]))\n",
    "gensim_1_dev = run_v5(dev_data, mapping_cluster2texts, mapping_text2cluster, 'dev')\n",
    "print(\"Dev accuracy: \" + str(evaluate(gensim_1_dev, gold_dev)[0]))\n",
    "gensim_1_test = run_v5(test_data, mapping_cluster2texts, mapping_text2cluster, 'test')\n",
    "print(\"Test accuracy: \" + str(evaluate(gensim_1_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following part is an attempt to improve an answering method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5. Choosing an Answer Based on Similarity\n",
    "- train doc2vec with sentences from texts, questions and answers\n",
    "- infer vectors for Q&As\n",
    "- score answers based on their similarity with the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_preprocessing(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = []\n",
    "    for sent in sentences:\n",
    "        tokenized_sentences.append(preprocess_text(sent))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml2listData_v2(xml_data):\n",
    "    data = []\n",
    "    root = ET.XML(xml_data)\n",
    "    for child in root:\n",
    "        for subchild in child:\n",
    "            if subchild.tag == 'text':\n",
    "                data.append(sentence_preprocessing(subchild.text))\n",
    "            if subchild.tag == 'questions':\n",
    "                for question in subchild:\n",
    "                    data.append(sentence_preprocessing(question.attrib[\"text\"]))\n",
    "                    for answer in question:\n",
    "                        data.append(sentence_preprocessing(answer.attrib[\"text\"]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_train = xml2listData_v2(xml_train)\n",
    "new_dev = xml2listData_v2(xml_dev)\n",
    "new_test = xml2listData_v2(xml_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for el in np.concatenate([new_train, new_dev, new_test]):\n",
    "    for s in el:\n",
    "        all_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Documents(object):\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            yield TaggedDocument(words = doc, tags = [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_sentences = Documents(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "count = len(all_sentences)\n",
    "model_sim = Doc2Vec(size=40, dbow_words= 1, dm=0, iter=1,  window=8, seed=1337, min_count=5, workers=4,alpha=0.025, min_alpha=0.025)\n",
    "model_sim.build_vocab(doc_sentences)\n",
    "for epoch in range(20):\n",
    "    print(\"epoch \"+str(epoch))\n",
    "    model_sim.train(doc_sentences, total_examples=count, epochs=1)\n",
    "    model_sim.save('commonSense_sim.model')\n",
    "    model_sim.alpha -= 0.002  # decrease the learning rate\n",
    "    model_sim.min_alpha = model_sim.alpha  # fix the learning rate, no decay\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring Vectors and Answering Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_answer_similarity(question, answers, model_sim):\n",
    "    question_vector = model_sim.infer_vector(question)\n",
    "    answer_scores = {}\n",
    "    for answer in answers:\n",
    "        answer_vector = model_sim.infer_vector(answer['answer_text'])\n",
    "        answer_scores[answer['answer_id']] = pairwise.cosine_similarity(question_vector.reshape(1, -1), answer_vector.reshape(1, -1))\n",
    "    best_answer = max(answer_scores.items(), key=lambda k: k[1])[0]\n",
    "    return best_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94402874"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_answer_similarity(new_train[1][0], new_train[2][0], model_sim)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v6(data, choose_answer_method, model):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer_method(question['question_text'], question['answers'], model)\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.121364710718325"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(run_v6(train_data, choose_answer_similarity, model_sim), ideal_results_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.82282069454288"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(run_v6(dev_data, choose_answer_similarity, model_sim), ideal_results_dev)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
