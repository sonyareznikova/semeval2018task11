{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import random as rd\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preprocessing Data\n",
    "Parsing xml train and dev data\n",
    "Tokenising text/questions/answers, getting rid of punctuation and upper cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml_train = open('MCScript/train-data.xml').read()\n",
    "xml_dev = open('MCScript/dev-data.xml').read()\n",
    "xml_test = open('MCScript/test-data.xml').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to preprocess data\n",
    "\n",
    "punctuation_and_extra = set(list('!\"#$%&\\'()*+,-./:;<=>? @[\\\\]^_`{|}~Â£'))\n",
    "punctuation_and_extra.update([\"'s\", \"n't\", \"``\", \"''\", \"'ll\", \"'m\", \"'d\", \"'ve\", \"wa\"])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokenised_text = []\n",
    "    for word in tokens:\n",
    "        if \":\" in word: #to deal with time formats, e.g. 5:00am\n",
    "            new_word = word.replace(\":\", \" \")\n",
    "            tokenised_text.append(new_word.lower())\n",
    "        else:\n",
    "            if (wn.morphy(word.lower()) != None):\n",
    "                tokenised_text.append(wn.morphy(word.lower()))\n",
    "            else:\n",
    "                tokenised_text.append(word.lower())\n",
    "    tokenised_text = list(filter(lambda x: x not in punctuation_and_extra, tokenised_text))\n",
    "    return tokenised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to read data from xml\n",
    "\n",
    "def xml2listData(xml_data):\n",
    "    data = []\n",
    "    root = ET.XML(xml_data)\n",
    "    for child in root:\n",
    "        instance = {}\n",
    "        for subchild in child:\n",
    "            if subchild.tag == 'text':\n",
    "                instance['text_id'] = child.attrib[\"id\"]\n",
    "                instance[subchild.tag] = preprocess_text(subchild.text)\n",
    "            if subchild.tag == 'questions':\n",
    "                questions = []\n",
    "                for question in subchild:\n",
    "                    single_question = {}\n",
    "                    single_question['question_id'] = question.attrib[\"id\"]\n",
    "                    single_question['question_text'] = preprocess_text(question.attrib[\"text\"])\n",
    "                    answers = []\n",
    "                    for answer in question:\n",
    "                        single_answer = {}\n",
    "                        single_answer['answer_id'] = answer.attrib[\"id\"]\n",
    "                        single_answer['answer_text'] = preprocess_text(answer.attrib[\"text\"])\n",
    "                        single_answer['correct'] = answer.attrib[\"correct\"]\n",
    "                        answers.append(single_answer)\n",
    "                    single_question['answers'] = answers\n",
    "                    questions.append(single_question)\n",
    "                instance['questions'] = questions\n",
    "        data.append(instance)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = xml2listData(xml_train)\n",
    "dev_data = xml2listData(xml_dev)\n",
    "test_data = xml2listData(xml_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing and preprocessing knowledge script data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to read DeScript xml\n",
    "\n",
    "def xml2listDeScript(xml_file):\n",
    "    data = []\n",
    "    root = ET.XML(xml_file)\n",
    "    for label in root:\n",
    "        for item in label:\n",
    "            data.append(preprocess_text(item.attrib[\"original\"]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "# returns a list of strings which are names of files\n",
    "first_gold_standard = glob.glob(\"./DeScript_LREC2016/gold_paraphrase_sets/first_gold_annotation/*.xml\")\n",
    "second_gold_standard = glob.glob(\"./DeScript_LREC2016/gold_paraphrase_sets/second_gold_annotation/*.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scriptFiles_to_lists(scriptFiles):\n",
    "    topcs = []\n",
    "    for filename in scriptFiles:\n",
    "        current_file = open(filename).read()\n",
    "        topcs.append(xml2listDeScript(current_file))\n",
    "    return topcs\n",
    "\n",
    "topcs1 = scriptFiles_to_lists(first_gold_standard)\n",
    "topcs2 = scriptFiles_to_lists(second_gold_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a dictionary with topics from both gold standards\n",
    "topics = {'baking_a_cake': np.concatenate(topcs1[0]+topcs2[0]),  'book_from_library': np.concatenate(topcs1[1]+topcs2[1]), 'airplane_flying': np.concatenate(topcs1[2]+topcs2[2]), \n",
    "          'hair_cut': np.concatenate(topcs1[3]+topcs2[3]), 'grocery_shopping': np.concatenate(topcs1[4]+topcs2[4]),'on_the_train': np.concatenate(topcs1[5]+topcs2[5]), \n",
    "          'planting_a_tree': np.concatenate(topcs1[6]+topcs2[6]), 'repair_flat_bike_tyre': np.concatenate(topcs1[7]+topcs2[7]), \n",
    "          'on_the_bus': np.concatenate(topcs1[8]+topcs2[8]), 'taking_bath': np.concatenate(topcs1[9]+topcs2[9])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a dictionary with docs within topics\n",
    "topic_docs = {'baking_a_cake': topcs1[0]+topcs2[0],  'book_from_library': topcs1[1]+topcs2[1], 'airplane_flying': topcs1[2]+topcs2[2], \n",
    "          'hair_cut': topcs1[3]+topcs2[3], 'grocery_shopping': topcs1[4]+topcs2[4],'on_the_train': topcs1[5]+topcs2[5], \n",
    "          'planting_a_tree': topcs1[6]+topcs2[6], 'repair_flat_bike_tyre': topcs1[7]+topcs2[7], \n",
    "          'on_the_bus': topcs1[8]+topcs2[8], 'taking_bath': topcs1[9]+topcs2[9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gold(data):\n",
    "    gold_standard = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            instance_result = [instance['text_id']]\n",
    "            instance_result.append(question['question_id'])\n",
    "            for answer in question['answers']:\n",
    "                if answer['correct'] == 'True':\n",
    "                    instance_result.append(answer['answer_id'])\n",
    "            gold_standard.append(instance_result)\n",
    "    return gold_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_train = get_gold(train_data)\n",
    "gold_dev = get_gold(dev_data)\n",
    "gold_test = get_gold(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluating accuracy, returning a tuple with percentage and list of incorrect answers\n",
    "def evaluate(results, ideal_results):\n",
    "    if (len(results) != len(ideal_results)):\n",
    "        raise Exception(\"Different length of your result and ideal results\")\n",
    "    correct_answers = 0\n",
    "    all_answers = len(results)\n",
    "    incorrect_answers = []\n",
    "    for i in range(all_answers):\n",
    "        if results[i] == ideal_results[i]:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            incorrect_answers.append(results[i])\n",
    "    return ((correct_answers/all_answers)*100, incorrect_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_results(results):\n",
    "    f = open('answer.txt', 'w')\n",
    "    for res in results:\n",
    "        f.write(\",\".join(res) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Questions\n",
    "- part 1: answer using only the given text\n",
    "- part 2: answer using other texts with similar topics\n",
    "- part 3: answer using text + knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Baseline. Answering Using The Given Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('917', '1', 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the longest answer length\n",
    "from operator import itemgetter\n",
    "all_answers = []\n",
    "for instance in train_data+dev_data:\n",
    "    for question in instance['questions']:\n",
    "        for answer in question['answers']:\n",
    "            all_answers.append((instance['text_id'], question['question_id'], len(answer['answer_text'])))\n",
    "max(all_answers,key=itemgetter(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_answer(text, answers):\n",
    "    answer_scores = {}\n",
    "    for answer in answers:\n",
    "        if \" \".join(answer['answer_text']) in \" \".join(text):\n",
    "            answer_scores[answer['answer_id']] = 28 #longest answer length\n",
    "        else:\n",
    "            clean_words = list(filter(lambda x: x not in stopwords, answer['answer_text']))\n",
    "            current_score = 0\n",
    "            for word in clean_words:\n",
    "                if word in text:\n",
    "                    current_score += 1\n",
    "            answer_scores[answer['answer_id']] = current_score \n",
    "    best_answer = max(answer_scores.items(), key=lambda k: k[1])\n",
    "    return best_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to answer yes-no questions\n",
    "def get_answer_for_general_form(question_text, text, answers):\n",
    "    correct_answer = 0\n",
    "    #leaving only meaningful words in the question\n",
    "    ques = list(filter(lambda x: x not in stopwords, question_text))\n",
    "    occurence_number = 0\n",
    "    for word in ques:\n",
    "        if word in text:\n",
    "            occurence_number += 1\n",
    "    if occurence_number > 2:\n",
    "        for answer in answers:\n",
    "            if 'yes' in answer['answer_text']:\n",
    "                return answer['answer_id']\n",
    "    else:\n",
    "        for answer in answers:\n",
    "            if 'yes' not in answer['answer_text']:\n",
    "                return answer['answer_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_general_question(answers):\n",
    "    answer_texts = np.concatenate([answer['answer_text'] for answer in answers])\n",
    "    if \"yes\" in answer_texts:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#taking into account \"YES/NO\" questions\n",
    "# TODO: add doc2vec similarity between question & answer\n",
    "def choose_answer_v3(text, question_text, answers):\n",
    "    answer_scores = {}\n",
    "    #checking if the question is a general one\n",
    "    is_general = is_general_question(answers)\n",
    "    if is_general:\n",
    "        return get_answer_for_general_form(question_text, text, answers)\n",
    "    #looking for answer\n",
    "    for answer in answers:\n",
    "        if \" \".join(answer['answer_text']) in \" \".join(text):\n",
    "            answer_scores[answer['answer_id']] = 28 #longest answer length\n",
    "        else:\n",
    "            clean_words = list(filter(lambda x: x not in stopwords, answer['answer_text']))\n",
    "            current_score = 0\n",
    "            for word in clean_words:\n",
    "                if word in text:\n",
    "                    current_score += 1\n",
    "            answer_scores[answer['answer_id']] = current_score \n",
    "    best_answer = max(answer_scores.items(), key=lambda k: k[1])\n",
    "    return best_answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assuming data in the train/dev format (dictionaries of dictionaries and lists)\n",
    "# returning results in the form of [[0,1,1],[0,2,1]...]\n",
    "def run(data):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer(instance['text'], question['answers'])[0]\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v4(data, choose_answer_method):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer_method(instance['text'], question['question_text'],question['answers'])\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline train: 60.20963929709177\n",
      "Baseline dev: 60.45357902197024\n",
      "Baseline test: 60.707901322845906\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline train: \" + str(evaluate(run(train_data), gold_train)[0]))\n",
    "print(\"Baseline dev: \" + str(evaluate(run(dev_data), gold_dev)[0]))\n",
    "print(\"Baseline test: \" + str(evaluate(run(test_data), gold_test)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yes-No Distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes-No for train data: 58.976466961257834\n",
      "Yes-No for dev data: 56.909992912827775\n",
      "Yes-No for test data: 59.52806578476939\n"
     ]
    }
   ],
   "source": [
    "# Yes-No Distinction\n",
    "res_gen = run_v4(train_data, choose_answer_v3)\n",
    "print(\"Yes-No for train data: \" + str(evaluate(res_gen, gold_train)[0]))\n",
    "res_gen_dev = run_v4(dev_data, choose_answer_v3)\n",
    "print(\"Yes-No for dev data: \" + str(evaluate(res_gen_dev, gold_dev)[0]))\n",
    "res_gen_test = run_v4(test_data, choose_answer_v3)\n",
    "print(\"Yes-No for test data: \" + str(evaluate(res_gen_test, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing Incorrect Answers for Two Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20', '5', '1'],\n",
       " ['20', '6', '1'],\n",
       " ['20', '7', '1'],\n",
       " ['20', '8', '1'],\n",
       " ['21', '0', '1']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(res_gen, gold_train)[1][32:37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['19', '6', '0'],\n",
       " ['20', '1', '0'],\n",
       " ['20', '3', '1'],\n",
       " ['20', '5', '1'],\n",
       " ['20', '6', '1'],\n",
       " ['20', '7', '1'],\n",
       " ['20', '8', '1']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(run(train_data), gold_train)[1][33:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Answering Using Same-Cluster Texts\n",
    "- Modelling topics of train and dev data with BigARTM\n",
    "- Using texts from the same cluster to find the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modelling with BigARTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform into suitable input format and getting rid of common English words in the text to aid topic modelling\n",
    "f = open(\"all_texts.txt\", \"w\")\n",
    "all_texts = []\n",
    "for instance in train_data:\n",
    "    clean_text = instance['text']\n",
    "    clean_text = list(filter(lambda x: x not in stopwords, clean_text))\n",
    "    all_texts.append([\"train\" + instance['text_id'] + \" |text \" + \" \".join(clean_text)])\n",
    "for instance in dev_data:\n",
    "    clean_text = instance['text']\n",
    "    clean_text = list(filter(lambda x: x not in stopwords, clean_text))\n",
    "    all_texts.append([\"dev\" + instance['text_id'] + \" |text \" + \" \".join(clean_text)])\n",
    "for instance in test_data:\n",
    "    clean_text = instance['text']\n",
    "    clean_text = list(filter(lambda x: x not in stopwords, clean_text))\n",
    "    all_texts.append([\"test\" + instance['text_id'] + \" |text \" + \" \".join(clean_text)])\n",
    "\n",
    "for text in all_texts:\n",
    "    f.write(text[0]+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorise\n",
    "import artm\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path=\"all_texts.txt\", data_format=\"vowpal_wabbit\", target_folder=\"text_batches\")\n",
    "\n",
    "# create model\n",
    "T = 100 #number of topics\n",
    "model_artm = artm.ARTM(num_topics=T, topic_names=[\"topic\"+str(i) for i in range(T)], class_ids={\"text\":1}, \n",
    "                       num_document_passes=2, reuse_theta=True, cache_theta=True, seed=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise model\n",
    "dictionary = artm.Dictionary('dictionary')\n",
    "dictionary.gather(batch_vectorizer.data_path)\n",
    "\n",
    "model_artm.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore', class_id=\"text\"))\n",
    "model_artm.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore'))\n",
    "model_artm.scores.add(artm.TopTokensScore(name=\"top_words\", num_tokens=15, class_id=\"text\"))\n",
    "model_artm.scores.add(artm.PerplexityScore(name=\"PerplexityScore\", dictionary='dictionary'))\n",
    "\n",
    "model_artm.initialize('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_artm.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_artm.theta_columns_naming = \"title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = model_artm.get_theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic0\n",
      "['door', 'phone', 'say', 'ring', 'hear', 'answer', 'call', 'man', 'open', 'package', 'front', 'doorbell', 'house', 'know', 'could']\n",
      "topic1\n",
      "['soup', 'pot', 'add', 'cook', 'make', 'put', 'chicken', 'rack', 'pan', 'dinner', 'dishwasher', 'ingredient', 'cooking', 'stove', 'start']\n",
      "topic2\n",
      "['plate', 'put', 'set', 'table', 'fork', 'one', 'napkin', 'place', 'silverware', 'cup', 'dinner', 'knife', 'eat', 'next', 'get']\n",
      "topic3\n",
      "['outside', 'work', 'get', 'sandwich', 'tom', 'go', 'pick', 'start', 'rain', 'put', 'would', 'today', 'going', 'ready', 'job']\n",
      "topic4\n",
      "['go', 'would', 'though', 'going', 'could', 'well', 'back', 'water', 'jessie', 'environment', 'love', 'like', 'float', 'dress', 'stay']\n",
      "topic5\n",
      "['dog', 'hot', 'put', 'cook', 'take', 'grill', 'mustard', 'bun', 'ketchup', 'buns', 'eat', 'make', 'microwave', 'place', 'package']\n",
      "topic6\n",
      "['battery', 'clock', 'time', 'alarm', 'go', 'late', 'compartment', 'work', 'know', 'home', 'back', 'morning', 'wake', 'lunch', 'right']\n",
      "topic7\n",
      "['dish', 'bread', 'toast', 'dry', 'put', 'toaster', 'sink', 'one', 'get', 'kitchen', 'pop', 'turn', 'take', 'silverware', 'butter']\n",
      "topic8\n",
      "['gas', 'car', 'pump', 'tank', 'station', 'pull', 'back', 'fuel', 'cap', 'nozzle', 'get', 'put', 'card', 'fill', 'truck']\n",
      "topic9\n",
      "['room', 'living', 'start', 'one', 'next', 'small', 'little', 'take', 'like', 'feel', 'dad', 'decide', 'time', 'put', 'finally']\n",
      "topic10\n",
      "['john', 'newspaper', 'dad', 'get', 'like', 'morning', 'go', 'love', 'start', 'work', 'house', 'kind', 'wake', 'every', 'new']\n",
      "topic11\n",
      "['wallpaper', 'read', 'cut', 'paper', 'get', 'would', 'go', 'take', 'able', 'apply', 'use', 'sweater', 'start', 'carefully', 'supply']\n",
      "topic12\n",
      "['dentist', 'teeth', 'go', 'tell', 'mouth', 'appointment', 'take', 'back', 'say', 'come', 'call', 'chair', 'room', 'name', 'office']\n",
      "topic13\n",
      "['plastic', 'camping', 'pack', 'set', 'bag', 'tent', 'camp', 'go', 'put', 'two', 'top', 'first', 'sleeping', 'decide', 'dinner']\n",
      "topic14\n",
      "['picnic', 'husband', 'decide', 'get', 'day', 'nice', 'basket', 'time', 'alex', 'blanket', 'great', 'going', 'potato', 'would', 'favorite']\n",
      "topic15\n",
      "['envelope', 'address', 'stamp', 'would', 'put', 'make', 'post', 'place', 'seal', 'office', 'could', 'mailbox', 'name', 'invitation', 'time']\n",
      "topic16\n",
      "['doctor', 'appointment', 'call', 'ask', 'take', 'room', 'check', 'office', 'tell', 'go', 'nurse', 'wait', 'come', 'blood', 'left']\n",
      "topic17\n",
      "['know', 'grandmother', 'like', 'going', 'cindy', 'take', 'things', 'time', 'would', 'show', 'seats', 'ever', 'lot', 'along', 'ha']\n",
      "topic18\n",
      "['us', 'tell', 'come', 'order', 'waitress', 'go', 'ask', 'back', 'minutes', 'two', 'husband', 'bring', 'make', 'take', 'ten']\n",
      "topic19\n",
      "['friend', 'pretty', 'piece', 'around', 'get', 'would', 'need', 'weekend', 'lot', 'keep', 'nice', 'important', 'fun', 'going', 'usually']\n",
      "topic20\n",
      "['painting', 'band', 'aid', 'hang', 'would', 'mark', 'finger', 'get', 'cut', 'one', 'put', 'place', 'sure', 'make', 'go']\n",
      "topic21\n",
      "['wash', 'put', 'bath', 'laundry', 'tub', 'water', 'clothes', 'dirty', 'dry', 'washing', 'load', 'take', 'finish', 'clean', 'bubble']\n",
      "topic22\n",
      "['pasta', 'sauce', 'boil', 'minutes', 'cook', 'add', 'water', 'put', 'heat', 'let', 'get', 'meat', 'first', 'need', 'make']\n",
      "topic23\n",
      "['table', 'mom', 'place', 'put', 'dinner', 'family', 'get', 'top', 'dishwasher', 'kitchen', 'dining', 'glasses', 'next', 'plate', 'clear']\n",
      "topic24\n",
      "['sponge', 'shower', 'scrub', 'sink', 'rinse', 'water', 'side', 'remove', 'use', 'spray', 'clean', 'band-aid', 'cleaning', 'way', 'soap']\n",
      "topic25\n",
      "['night', 'get', 'sleep', 'bed', 'take', 'go', 'ready', 'last', 'fall', 'little', 'time', 'asleep', 'brush', 'teeth', 'help']\n",
      "topic26\n",
      "['water', 'shower', 'get', 'soap', 'rinse', 'towel', 'turn', 'body', 'hot', 'take', 'bucket', 'dry', 'clean', 'start', 'warm']\n",
      "topic27\n",
      "['light', 'bulb', 'switch', 'back', 'one', 'turn', 'new', 'go', 'old', 'make', 'lamp', 'sure', 'screw', 'get', 'ladder']\n",
      "topic28\n",
      "['breakfast', 'toast', 'eggs', 'bacon', 'morning', 'cook', 'butter', 'make', 'slice', 'wake', 'eat', 'two', 'bread', 'flip', 'toaster']\n",
      "topic29\n",
      "['beach', 'water', 'dad', 'sand', 'go', 'get', 'towel', 'pack', 'ocean', 'could', 'umbrella', 'bring', 'take', 'house', 'swim']\n",
      "topic30\n",
      "['check', 'line', 'make', 'sure', 'go', 'plane', 'put', 'take', 'one', 'airport', 'get', 'security', 'back', 'gate', 'time']\n",
      "topic31\n",
      "['pay', 'bill', 'month', 'check', 'payment', 'money', 'one', 'company', 'account', 'card', 'get', 'credit', 'water', 'write', 'mail']\n",
      "topic32\n",
      "['flight', 'go', 'trip', 'ticket', 'airport', 'plane', 'fly', 'vacation', 'airline', 'want', 'decide', 'time', 'florida', 'travel', 'day']\n",
      "topic33\n",
      "['pan', 'eggs', 'milk', 'heat', 'stove', 'egg', 'turn', 'make', 'pour', 'cook', 'add', 'hot', 'get', 'omelette', 'bowl']\n",
      "topic34\n",
      "['water', 'stove', 'pot', 'pour', 'place', 'turn', 'heat', 'put', 'minutes', 'fill', 'boiling', 'get', 'noodle', 'open', 'grab']\n",
      "topic35\n",
      "['fish', 'medicine', 'tank', 'food', 'top', 'take', 'cabinet', 'water', 'see', 'go', 'make', 'give', 'back', 'small', 'sure']\n",
      "topic36\n",
      "['plant', 'garden', 'grow', 'hole', 'water', 'seed', 'tree', 'soil', 'would', 'make', 'get', 'dirt', 'work', 'tomato', 'day']\n",
      "topic37\n",
      "['movie', 'watch', 'go', 'theater', 'dvd', 'want', 'decide', 'get', 'rent', 'play', 'popcorn', 'player', 'ticket', 'pick', 'home']\n",
      "topic38\n",
      "['clothes', 'fold', 'put', 'clothing', 'sock', 'shirt', 'pants', 'laundry', 'pair', 'folding', 'pile', 'first', 'take', 'together', 'top']\n",
      "topic39\n",
      "['floor', 'clean', 'mop', 'cleaner', 'toilet', 'bathroom', 'use', 'cleaning', 'sweep', 'wipe', 'spray', 'dust', 'get', 'make', 'first']\n",
      "topic40\n",
      "['bag', 'garbage', 'trash', 'take', 'open', 'top', 'item', 'place', 'full', 'pull', 'close', 'new', 'kitchen', 'put', 'grab']\n",
      "topic41\n",
      "['orange', 'juice', 'bottle', 'fresh', 'get', 'juicer', 'make', 'drink', 'put', 'squeeze', 'half', 'formula', 'go', 'cut', 'take']\n",
      "topic42\n",
      "['funeral', 'family', 'make', 'take', 'people', 'camera', 'sad', 'black', 'look', 'us', 'friend', 'flower', 'day', 'things', 'stand']\n",
      "topic43\n",
      "['shoes', 'bowling', 'go', 'game', 'ball', 'get', 'friend', 'alley', 'lane', 'would', 'us', 'pin', 'size', 'one', 'put']\n",
      "topic44\n",
      "['sauna', 'steak', 'relax', 'go', 'bring', 'back', 'steam', 'hot', 'order', 'would', 'minutes', 'take', 'felt', 'room', 'could']\n",
      "topic45\n",
      "['play', 'tennis', 'ball', 'battery', 'court', 'clock', 'alarm', 'game', 'back', 'go', 'hit', 'get', 'playing', 'time', 'set']\n",
      "topic46\n",
      "['baby', 'diaper', 'change', 'wipe', 'put', 'clean', 'take', 'back', 'would', 'get', 'crying', 'sure', 'need', 'make', 'new']\n",
      "topic47\n",
      "['book', 'read', 'go', 'child', 'found', 'story', 'reading', 'like', 'want', 'look', 'bring', 'day', 'check', 'get', 'make']\n",
      "topic48\n",
      "['go', 'front', 'see', 'row', 'tell', 'look', '911', 'could', 'able', 'help', 'one', 'woman', 'house', 'hear', 'police']\n",
      "topic49\n",
      "['take', 'picture', 'online', 'get', 'want', 'camera', 'information', 'look', 'like', 'go', 'could', 'phone', 'price', 'website', 'see']\n",
      "topic50\n",
      "['kid', 'back', 'inside', 'bag', 'get', 'end', 'put', 'make', 'big', 'around', 'tie', 'keep', 'would', 'one', 'school']\n",
      "topic51\n",
      "['coffee', 'filter', 'water', 'make', 'maker', 'pour', 'cup', 'put', 'machine', 'grounds', 'pot', 'add', 'take', 'hot', 'morning']\n",
      "topic52\n",
      "['trip', 'pack', 'suitcase', 'need', 'go', 'take', 'get', 'would', 'day', 'everything', 'sure', 'bring', 'also', 'prepare', 'things']\n",
      "topic53\n",
      "['pool', 'towel', 'swimming', 'go', 'get', 'bathroom', 'day', 'put', 'water', 'lightbulb', 'sunscreen', 'swim', 'walk', 'hot', 'take']\n",
      "topic54\n",
      "['food', 'cat', 'order', 'eat', 'get', 'take', 'go', 'restaurant', 'fast', 'give', 'place', 'water', 'want', 'tell', 'feed']\n",
      "topic55\n",
      "['hair', 'shampoo', 'conditioner', 'get', 'cut', 'wash', 'head', 'back', 'dry', 'haircut', 'make', 'rinse', 'time', 'chair', 'take']\n",
      "topic56\n",
      "['clean', 'cleaning', 'put', 'sink', 'back', 'go', 'bathroom', 'dirty', 'left', 'take', 'mess', 'away', 'make', 'spray', 'dishwasher']\n",
      "topic57\n",
      "['tree', 'paint', 'hole', 'want', 'would', 'get', 'water', 'large', 'newspaper', 'sure', 'fill', 'new', 'around', 'dirt', 'go']\n",
      "topic58\n",
      "['vegetable', 'cut', 'chop', 'cutting', 'knife', 'onion', 'board', 'get', 'make', 'put', 'piece', 'first', 'salad', 'slice', 'wash']\n",
      "topic59\n",
      "['felt', 'last', 'really', 'take', 'better', 'time', 'get', 'enjoy', 'would', 'experience', 'daughter', 'good', 'day', 'great', 'want']\n",
      "topic60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cake', 'oven', 'mix', 'bake', 'ingredient', 'baking', 'pan', 'put', 'need', 'make', 'get', 'chocolate', 'frosting', 'recipe', 'minutes']\n",
      "topic61\n",
      "['wedding', 'bride', 'groom', 'ceremony', 'church', 'carrot', 'married', 'take', 'aisle', 'everyone', 'dress', 'couple', 'reception', 'exchange', 'celery']\n",
      "topic62\n",
      "['button', 'thread', 'needle', 'shirt', 'one', 'push', 'hole', 'hit', 'miss', 'end', 'tie', 'find', 'put', 'times', 'found']\n",
      "topic63\n",
      "['job', 'get', 'make', 'show', 'interview', 'since', 'way', 'take', 'always', 'work', 'tell', 'little', 'notice', 'would', 'decide']\n",
      "topic64\n",
      "['museum', 'go', 'walk', 'exhibit', 'could', 'dinosaur', 'saw', 'brother', 'ticket', 'see', 'mom', 'old', 'wait', 'dad', 'even']\n",
      "topic65\n",
      "['would', 'want', 'reservation', 'ask', 'restaurant', 'could', 'make', 'dinner', 'thought', 'number', 'time', 'agree', 'decide', 'call', 'like']\n",
      "topic66\n",
      "['machine', 'vending', 'chips', 'snack', 'dollar', 'money', 'get', 'buy', 'store', 'want', 'candy', 'decide', 'found', 'go', 'put']\n",
      "topic67\n",
      "['things', 'bowl', 'like', 'cat', 'ha', 'back', 'food', 'make', 'store', 'go', 'place', 'time', 'big', 'fresh', 'sure']\n",
      "topic68\n",
      "['clean', 'use', 'time', 'process', 'easy', 'put', 'shower', 'feel', 'let', 'first', 'usually', 'one', 'start', 'wall', 'cleaner']\n",
      "topic69\n",
      "['car', 'drive', 'turn', 'get', 'put', 'make', 'back', 'drove', 'key', 'sure', 'parking', 'right', 'park', 'road', 'side']\n",
      "topic70\n",
      "['walk', 'door', 'get', 'home', 'go', 'leash', 'around', 'open', 'front', 'back', 'walking', 'saw', 'outside', 'start', 'time']\n",
      "topic71\n",
      "['clothes', 'machine', 'put', 'laundry', 'washing', 'basket', 'dryer', 'washer', 'detergent', 'wash', 'dirty', 'take', 'start', 'dry', 'add']\n",
      "topic72\n",
      "['tea', 'water', 'mug', 'cup', 'kettle', 'bag', 'put', 'take', 'hot', 'make', 'drink', 'teabag', 'pour', 'teaspoon', 'get']\n",
      "topic73\n",
      "['fish', 'oil', 'make', 'one', 'good', 'put', 'doe', 'piece', 'pan', 'cook', 'also', 'sure', 'time', 'take', 'little']\n",
      "topic74\n",
      "['look', 'sit', 'nice', 'make', 'get', 'good', 'sure', 'table', 'decide', 'bring', 'put', 'grab', 'time', 'one', 'want']\n",
      "topic75\n",
      "['tire', 'bike', 'back', 'tube', 'shirt', 'put', 'air', 'get', 'ride', 'use', 'bicycle', 'repair', 'kit', 'take', 'flat']\n",
      "topic76\n",
      "['train', 'station', 'ticket', 'get', 'take', 'york', 'new', 'city', 'walk', 'arrive', 'stop', 'car', 'seat', 'ride', 'wait']\n",
      "topic77\n",
      "['barbecue', 'make', 'people', 'everyone', 'decide', 'well', 'like', 'choice', 'start', 'day', 'meat', 'enjoy', 'cup', 'good', 'sure']\n",
      "topic78\n",
      "['box', 'toy', 'put', 'paper', 'gift', 'tape', 'wrapping', 'wrap', 'make', 'would', 'fold', 'piece', 'side', 'pick', 'one']\n",
      "topic79\n",
      "['salmon', 'get', 'go', 'park', 'minutes', 'back', 'salad', 'buy', 'work', 'also', 'crowd', 'slow', 'put', 'make', 'let']\n",
      "topic80\n",
      "['fire', 'wood', 'start', 'bonfire', 'log', 'go', 'make', 'catch', 'dry', 'around', 'pit', 'lighter', 'place', 'friend', 'get']\n",
      "topic81\n",
      "['paper', 'gift', 'buy', 'store', 'go', 'get', 'birthday', 'home', 'shop', 'grocery', 'present', 'wrap', 'need', 'purchase', 'wrapping']\n",
      "topic82\n",
      "['bed', 'sheet', 'pillow', 'put', 'blanket', 'make', 'top', 'take', 'corner', 'fit', 'tuck', 'place', 'mattress', 'side', 'pull']\n",
      "topic83\n",
      "['letter', 'write', 'envelope', 'mail', 'address', 'put', 'friend', 'stamp', 'pen', 'corner', 'life', 'writing', 'paper', 'right', 'top']\n",
      "topic84\n",
      "['driving', 'lesson', 'get', 'instructor', 'take', 'car', 'go', 'really', 'start', 'learn', 'tell', 'nervous', 'park', 'drive', 'driver']\n",
      "topic85\n",
      "['mother', 'special', 'table', 'dinner', 'grandmother', 'billy', 'come', 'time', 'get', 'together', 'flower', 'sister', 'help', 'hold', 'clean']\n",
      "topic86\n",
      "['list', 'shopping', 'store', 'need', 'item', 'grocery', 'go', 'things', 'get', 'cart', 'make', 'week', 'put', 'check', 'buy']\n",
      "topic87\n",
      "['bus', 'driver', 'get', 'stop', 'taxi', 'take', 'minutes', 'ride', 'pay', 'need', 'street', 'wait', 'people', 'cab', 'go']\n",
      "topic88\n",
      "['pizza', 'order', 'minutes', 'phone', 'decide', 'would', 'tell', 'want', 'call', 'give', 'get', 'delivery', 'ask', 'tip', 'deliver']\n",
      "topic89\n",
      "['book', 'library', 'card', 'librarian', 'borrow', 'need', 'back', 'looking', 'take', 'ask', 'computer', 'give', 'would', 'scan', 'research']\n",
      "topic90\n",
      "['wall', 'paint', 'room', 'new', 'floor', 'look', 'decide', 'house', 'want', 'put', 'buy', 'color', 'get', 'painting', 'work']\n",
      "topic91\n",
      "['would', 'plane', 'get', 'seat', 'could', 'time', 'ticket', 'take', 'grill', 'us', 'found', 'airport', 'decide', 'window', 'going']\n",
      "topic92\n",
      "['friend', 'dance', 'go', 'arrive', 'drink', 'time', 'dancing', 'great', 'fun', 'decide', 'one', 'good', 'home', 'music', 'going']\n",
      "topic93\n",
      "['go', 'get', 'visit', 'talk', 'house', 'home', 'see', 'family', 'cousin', 'would', 'time', 'sister', 'going', 'say', 'really']\n",
      "topic94\n",
      "['party', 'invitation', 'invite', 'birthday', 'friend', 'go', 'people', 'store', 'list', 'would', 'want', 'buy', 'get', 'decide', 'pick']\n",
      "topic95\n",
      "['vacuum', 'carpet', 'closet', 'get', 'plug', 'room', 'back', 'sure', 'make', 'house', 'turn', 'cord', 'move', 'furniture', 'living']\n",
      "topic96\n",
      "['iron', 'ironing', 'shirt', 'board', 'pants', 'wrinkle', 'clothes', 'get', 'put', 'heat', 'take', 'set', 'steam', 'need', 'place']\n",
      "topic97\n",
      "['family', 'would', 'date', 'go', 'nice', 'like', 'person', 'people', 'see', 'time', 'good', 'make', 'one', 'take', 'know']\n",
      "topic98\n",
      "['make', 'would', 'jenny', 'time', 'day', 'want', 'next', 'decide', 'live', 'weekend', 'chili', 'restaurant', 'problem', 'spend', 'call']\n",
      "topic99\n",
      "['lawn', 'mower', 'cup', 'mow', 'grass', 'yard', 'back', 'start', 'gas', 'go', 'pull', 'put', 'turn', 'push', 'cut']\n"
     ]
    }
   ],
   "source": [
    "# top words for each topic\n",
    "for topic in model_artm.score_tracker[\"top_words\"].last_tokens:\n",
    "    print(topic)\n",
    "    print(model_artm.score_tracker[\"top_words\"].last_tokens[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model_artm.save(\"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic33</th>\n",
       "      <td>0.024480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic90</th>\n",
       "      <td>0.021954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic26</th>\n",
       "      <td>0.020483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic37</th>\n",
       "      <td>0.017545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic82</th>\n",
       "      <td>0.016841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>0.016512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic39</th>\n",
       "      <td>0.015871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic54</th>\n",
       "      <td>0.014736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic32</th>\n",
       "      <td>0.014322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic83</th>\n",
       "      <td>0.014007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic71</th>\n",
       "      <td>0.013946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic86</th>\n",
       "      <td>0.013887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic93</th>\n",
       "      <td>0.013834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic70</th>\n",
       "      <td>0.013819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic87</th>\n",
       "      <td>0.013767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic0</th>\n",
       "      <td>0.013565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic92</th>\n",
       "      <td>0.013482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic55</th>\n",
       "      <td>0.013281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic69</th>\n",
       "      <td>0.013178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic65</th>\n",
       "      <td>0.012842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic76</th>\n",
       "      <td>0.012278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic21</th>\n",
       "      <td>0.012199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic49</th>\n",
       "      <td>0.012004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic38</th>\n",
       "      <td>0.011728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic23</th>\n",
       "      <td>0.011491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic97</th>\n",
       "      <td>0.011441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic78</th>\n",
       "      <td>0.011415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic12</th>\n",
       "      <td>0.011320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic45</th>\n",
       "      <td>0.011252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic94</th>\n",
       "      <td>0.011047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic30</th>\n",
       "      <td>0.007889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic24</th>\n",
       "      <td>0.007731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic48</th>\n",
       "      <td>0.007701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic96</th>\n",
       "      <td>0.007619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic64</th>\n",
       "      <td>0.007445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic9</th>\n",
       "      <td>0.007205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic72</th>\n",
       "      <td>0.007144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic68</th>\n",
       "      <td>0.007072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic89</th>\n",
       "      <td>0.007049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic73</th>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic13</th>\n",
       "      <td>0.006954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic6</th>\n",
       "      <td>0.006802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic63</th>\n",
       "      <td>0.006765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic67</th>\n",
       "      <td>0.006708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic10</th>\n",
       "      <td>0.006705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic98</th>\n",
       "      <td>0.006322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic57</th>\n",
       "      <td>0.006183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic91</th>\n",
       "      <td>0.006142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic11</th>\n",
       "      <td>0.006021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic14</th>\n",
       "      <td>0.005858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic77</th>\n",
       "      <td>0.005669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic61</th>\n",
       "      <td>0.005651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic85</th>\n",
       "      <td>0.005610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic15</th>\n",
       "      <td>0.005428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic79</th>\n",
       "      <td>0.005404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic50</th>\n",
       "      <td>0.005338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic3</th>\n",
       "      <td>0.004846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic19</th>\n",
       "      <td>0.004677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic4</th>\n",
       "      <td>0.004610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic17</th>\n",
       "      <td>0.004173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Probability\n",
       "topic33     0.024480\n",
       "topic90     0.021954\n",
       "topic26     0.020483\n",
       "topic37     0.017545\n",
       "topic82     0.016841\n",
       "topic1      0.016512\n",
       "topic39     0.015871\n",
       "topic54     0.014736\n",
       "topic32     0.014322\n",
       "topic83     0.014007\n",
       "topic71     0.013946\n",
       "topic86     0.013887\n",
       "topic93     0.013834\n",
       "topic70     0.013819\n",
       "topic87     0.013767\n",
       "topic0      0.013565\n",
       "topic92     0.013482\n",
       "topic55     0.013281\n",
       "topic69     0.013178\n",
       "topic65     0.012842\n",
       "topic76     0.012278\n",
       "topic21     0.012199\n",
       "topic49     0.012004\n",
       "topic38     0.011728\n",
       "topic23     0.011491\n",
       "topic97     0.011441\n",
       "topic78     0.011415\n",
       "topic12     0.011320\n",
       "topic45     0.011252\n",
       "topic94     0.011047\n",
       "...              ...\n",
       "topic30     0.007889\n",
       "topic24     0.007731\n",
       "topic48     0.007701\n",
       "topic96     0.007619\n",
       "topic64     0.007445\n",
       "topic9      0.007205\n",
       "topic72     0.007144\n",
       "topic68     0.007072\n",
       "topic89     0.007049\n",
       "topic73     0.007000\n",
       "topic13     0.006954\n",
       "topic6      0.006802\n",
       "topic63     0.006765\n",
       "topic67     0.006708\n",
       "topic10     0.006705\n",
       "topic98     0.006322\n",
       "topic57     0.006183\n",
       "topic91     0.006142\n",
       "topic11     0.006021\n",
       "topic14     0.005858\n",
       "topic77     0.005669\n",
       "topic61     0.005651\n",
       "topic85     0.005610\n",
       "topic15     0.005428\n",
       "topic79     0.005404\n",
       "topic50     0.005338\n",
       "topic3      0.004846\n",
       "topic19     0.004677\n",
       "topic4      0.004610\n",
       "topic17     0.004173\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_probabilities = []\n",
    "for i, topic in enumerate(model_artm.topic_names):\n",
    "    topic_probabilities.append(sum(theta.as_matrix()[i])/len(theta.as_matrix()[i]))\n",
    "\n",
    "probability_df = pd.DataFrame(columns=[\"Probability\"], index=model_artm.topic_names, data=topic_probabilities)\n",
    "probability_df.sort_values(\"Probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that __topic4__ has the biggest probability across all documents, especially compared to all others: around 69%. From top tokens, contained in this topic, we see that it's a topic with commonly used verbs and other parts of speech. Other similar topics include __topic0__ and __topic10__. Therefore, I can try to take the next best possibility for each document, disregarding the one for these three topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_by_id(text_id, data):\n",
    "    text = {}\n",
    "    for instance in data:\n",
    "        if instance['text_id'] == text_id:\n",
    "            text['text_id'] = text_id\n",
    "            text['text'] = instance['text']\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mapping to separate data-s\n",
    "train_topic2docs = {}\n",
    "dev_topic2docs = {}\n",
    "test_topic2docs = {}\n",
    "for col in theta.columns:\n",
    "    max_proba = max(theta.loc[:,col])\n",
    "    best_topic = theta.loc[theta[col] == max_proba].index\n",
    "    if 'train' in col:\n",
    "        current_text = get_text_by_id(col[5:], train_data)\n",
    "        if best_topic[0] not in train_topic2docs:\n",
    "            train_topic2docs[best_topic[0]] = []\n",
    "            train_topic2docs[best_topic[0]].append(current_text)\n",
    "        else:\n",
    "            train_topic2docs[best_topic[0]].append(current_text)\n",
    "    elif 'dev' in col:\n",
    "        current_text = get_text_by_id(col[3:], dev_data)\n",
    "        if best_topic[0] not in dev_topic2docs:\n",
    "            dev_topic2docs[best_topic[0]] = []\n",
    "            dev_topic2docs[best_topic[0]].append(current_text)\n",
    "        else:\n",
    "            dev_topic2docs[best_topic[0]].append(current_text)\n",
    "    elif 'test' in col:\n",
    "        current_text = get_text_by_id(col[4:], test_data)\n",
    "        if best_topic[0] not in test_topic2docs:\n",
    "            test_topic2docs[best_topic[0]] = []\n",
    "            test_topic2docs[best_topic[0]].append(current_text)\n",
    "        else:\n",
    "            test_topic2docs[best_topic[0]].append(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mapping from topics to texts\n",
    "# {topic: [{text_id:text, ...}], topic: ...}\n",
    "topic2docs = {}\n",
    "for col in theta.columns:\n",
    "    max_proba = max(theta.loc[:,col])\n",
    "    best_topic = theta.loc[theta[col] == max_proba].index\n",
    "    if best_topic[0] not in topic2docs:\n",
    "        topic2docs[best_topic[0]] = []\n",
    "        topic2docs[best_topic[0]].append(col)\n",
    "    else:\n",
    "        topic2docs[best_topic[0]].append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112795a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = [len(value) for value in topic2docs.values()]\n",
    "plt.barh([i for i in range(len(topic2docs.keys()))], lens)\n",
    "plt.savefig(\"fig_0.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the new distribution for document per topic. __Topic7__ is prevalent but it's not as bad as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 96 artists>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXmU3GWZtq+nOyEkIRCSsARCCEuQJRCQhgEmg4jigjqO\ngjouCAoqCg6KOviN+wgODAoqozAqbigq4MKgKAOyuA1KAoGwCgYCJGQhkARIAlme74/7LapSXZ3q\nJJ0qOn1f5/T5Vf22eqtOn3rqftbITIwxxphmdLR7AcYYY/oHNhjGGGN6hQ2GMcaYXmGDYYwxplfY\nYBhjjOkVNhjGGGN6hQ2GMcaYXmGDYYwxplfYYBhjjOkVg9q9gL5kzJgxOWHChHYvwxhj+hXTpk17\nPDO3aXbeJmUwJkyYwNSpU9u9DGOM6VdExKzenGeXlDHGmF5hg2GMMaZX2GAYY4zpFTYYxhhjeoUN\nhjHGmF6xVoMRESMj4gPre/OIuDoiRq7l+OERcWtErIyIY+uO/WdE3BUR90TEVyMi1ncdxhhjNpxm\nCmMksN4GIzOPzsxFaznlYeAE4NLanRFxGPD3wH7AJOAg4CXruw5jjDEbTrM6jC8Be0TEdODasu/V\nQAJnZuZPIuII4N+Bp4DdgRuAD2Tm6ohYCkzKzJkR8U7go+XaOzLzOGA18BVkGF4SEUsz8+pyzm7A\nDGTUtgbm9dWbNsYYs+40MxjnA6/JzP0j4hjgZGAyMAa4JSJ+V847GNgbmAX8BngjcAUwH1gSEfsA\nnwQOy8zHI2JUue6TwGXluunA14EJQACrgB3K46eA7YB71rbYGbMXM+Hjv+rdO6/hobNfs87XGGPM\nQKOZwTgD2KwojJXAAuB2pAAeQq6iXYHlwAVIYcwBpiCDMQ4YBRwJ3AtcHxEJ3AEcV+6zZXmtYeVa\nkKHYAqmMAB4s9+pGRLwXeC9A55ZNK9uNMcasJ80MxteAVxWFcRUyDvshhTETuYpAX/ofRErhDmQ4\nQO6kEcD2KCbxojqF8U3gRmBoef7hst2lXFdxQz2JjEg3MvMbwDcAhoydmE3eT0PWR5WAlYkxZmDR\nzGB01J1T+UJeXR5nzf6s21c576kG11e4ALmwnkZG4YsR8V/As+X4KKQwZpVtN6wwjDGmNTQzGB8E\niIhlwGJgKfBMOfYgsAjFLxL4M8qqWkA1QN4J7ATMRV/6s4tLag5yN+1bru8AlpX1jAE2L/f8C1I1\nj9Pd2AB9ozCMMcY0p1la7QUAmTkU+D5yPY0AXgxMRHEMkGE4FsUhhqB4Byhw/QgyFpOAfcu9jirH\n70AGZikyCAvL84qi2BoZqsH0EPCOiPdGxNSImLpq6eJevm1jjDHryroqjCVUXUz3o4ymHdGX/RVU\nFcbgck5FYewM3AnMqFMYS5DbqQO5r55DhuIK4J3APwDDy73ubrTAvlAYjkUYY0xzmimMi4EsquAW\n5DYajgzAeKQAZqMv+UNQ8HoR1aB3RWFsX64ZV+51UDm+MzIcPwDOK/cbgwzQcOBy5JZaDBzQaIFW\nGMYY0xqaKYyTgShptVsAW6G02g6kHhIpDIBvlcdjUHwDqgoD5Kq6KSJWohTbt5T9NyNDs7ycvwAV\n+E0ADkO1HMPZiArDWVLGGNOcZgrjorKtGJZosJ1dd019NtPTDV5vVdmegzKitkKFedMzM4GXIwXT\ngYxQJ3JRdcMKwxhjWkNvFAZUg9jZYLsja1L/K3+Lmsery7azbEchZbEYGY7dyv6rkCLZudx/CfD/\nGi2wVmF0dXXlVP/qN8aYjUIzg3ER8DLWrjDqWReFsRSpiyEo+6ryOq8DDkW1GR3IsPwQeFu3F6ur\nw1hf99L6YJeUMWYg0RcKo551URjbAE+U+28HrCj7Pwv8HKmPleW6zzdaoBWGMca0hnYrjB2QstgO\nqY0lNfdYhQLog5DhaNh8sJ0KY32xMjHG9EeaGYyTyrYnhfHSBtesLVPpeYUREVcD/wG8AvgEUjN/\nK8crzQffDnwOdcidgvpOrfliVhjGGNMSmhmMM1FVdiOFESib6Za6a9Y2Ge95hZGZR0fEdcBY4Fxk\nlB4ux3cp25+X+yXQ1WSt693evNVYYRhj+iPNDMbT6Mt9UulWuwv6tT8KeAy5iaaUcx8AtkXtQcaW\nfatRe5GtUa3FzcCJwMgyXOnN5Z4XoaryAyLiEmSEOlF84ymULVXr2nqeWpfU+PHj/WVsjDEbiVDZ\nQw8HIyagwUaPojjCAlSIlyjGsDdyB52MMpqeQI0C9wVeVK5bAZwNHIhcWI8j19IRqGfUPsj9dD+w\nJzJCJwHvQim1jyKDsSAzJ6ztzQwZOzHHHv/lXr/5/oaNoTFmYxAR0zKzqRenmcIIqgrjjcD7qCqM\nWeX6NyA1cBcKUu+H2oEchIzKc8hQHIo62k5DI13nl8fLUe+pPcv9vgL8qjy/p1w/BAXFG71RKwxj\njGkBzQxGogD1ncCvUXfZysS9s4F/Q/GH/0QKYg7wM9S5dmvkRhqJ1MLhyPU0ueyjHB+CWn/MKvc+\njWrgfA+qKbijI6IrM6euscC61iAe0WqMMRuHZgbjSygmsbLBseWs2Vqk0TClLRrsS6pG4G5kXEAN\nCq8EyMwnI2IVMih/QDMxZtYbC7DCMMaYVtEshjEFuC4zN4+IY1Cs4lXI9XQrMghL0Zf9TWX/zsgl\n9XHgu8g4TEcuqb+ggPbnkUvqE8DRyO10BDIgt2XmCcVgPITcX7OBCZlZWwTYjVbHMGycjDGbAn0V\nw/gEsFnpVrsSBb0rLqlZqI35OcBZKHj9RDm+HzIOI8u516N+US8FxiHD8iiwGQqOj6baL+rm8to/\nQnPAB5VrKh1w69+oR7QaY0wLaKYwjgUuy8yOkla7KzIGY4CZKKYxASmMb6J25LsDd2XmgUUlPIUM\nza7AJcC7kSG5FSmHSSjldiUyDpeiORgXorTcUWhk66cz8+y1vRlnSRljzLrTW4XRrL157TyMvVCd\nxe3ADcjVdCHw03Lu7ijb6RlUo1G5/z3AZWj40kHAbcCFJUV2JOpSC4qJDMrME1AAfRoyIJ3ImFTO\nq3+jbm9ujDEtoBW9pFbVPK7vJfVyFN9YjlREZf9K4PWo8jupZlJ1oy8GKK0v/sVvjBlI9EZhwIbN\nw/hMzeP6brUfQy6rv1X2RURH2bcMVYxvDixEKbbdsMIwxpjW0O5ute9Hwe7naq4bg1J5Hy/bLZAr\nbFyjBfaFwrBSMMaY5vSFwqhnXeZhVAYojUSKIlCAfDXwAzSB76lyzz82WqAVhjHGtIZ2K4z/Rm6p\n7ZFR+JfMzIiYh1J6z0DGZQGq0ehGXyiM/tDhFqyEjDHtpd0KYx+kLFZS2oKUGMYvyznPoAK/SqFg\nN6wwjDGmNbRbYRxV9i1Crqc9kHGYiAzGQlSjsQL4ZzRMaQ2cJWWMMa2h3TO9O1Bb9HmoAHA4cj99\nvZyztKxxEKrf6IYrvY0xpjU0c0ldVLYbS2HcSDXYvXnZNwql6g5CxYI7lusajYMlM7+RmV2Z2dU5\nbKu1vRdjjDEbQDOD0Wymd28URi31CuPjqHBvNWoTUhnMdBzwG2AEUjkrUW+qbjiGYYwxrWFDZnqD\n4hv19Gqmd0RcDdwLHIKaDyawMDMXRcR7UCuQB5EB6QCOQQ0J16CdMYz+kl1luuP4kzHrTjOF8fxM\nb9QTai4agHQ4UgVHImUAmtW9FxqiVGEVilGADMCUzJwMnJqZRwNfAMZl5hCkKCrXPoy62P4Sdbm9\nFfheowVaYRhjTGtopjBWAyvLxL3laM5Fpb3506gC+y3lvOtQNfYK1Poc5Hr6EeoFNQS4PyIeAW6L\niCPKsQ9HxJByv2ERcQlwAnB62a5E1eAzGi2wnQpjffGvW2NMf6SZwuhAHWQnITXQgRTGkaidx4No\nFgZIHQxHA5IOjoixKPtpH+Aj5fgdaHjS6eWa/wPeilJqL0aZUKchQzMJZUndC/wOdcLthhWGMca0\nhmYK41Nopvcyqr/wnynbS4GDUVtzkCFZCtyP3EsHAduhCXrXRMSNaLb3FKoK5C1oROtg4D3A6zLz\niYj4V9RHanm5/1jgh+X11qA/Koz1jX1YmRhj2kkzhXE+sCIzh6LJeouRitgZeCUyOA+gQPeZqMhu\nDjC+XD8PWBYR+6ARrB8BLkDGBFSjcRkyRucigwLKjnocGZ6/IjfXixot0ArDGGNaQ1+MaN0dxTA+\nhgLfC6iOaN0WNRY8FrmWzkLV22PK/fdB8YlJKMaxY0SMQgZpy3LeZmW7qNECaxVGV1dXTvWvcGOM\n2Sg0MxgXA6/MzP3XMqL1GBRz+Dka0dqFRrTOjYhE6uEdNB7Rujma4f0MsC9SHF9DsY13IKVS6Vb7\naKMF1ld694dUV7uWjDH9kd60BqmMaN0CtSK/HbmyKiNaDwZOoecRrSuR2+k0qiNa783MCRFxH6rB\n2L/cP9GMjHHluqfLfZehIUvdsMIwxpjW0O4RrQ8Ch6IA92qqhmh3pEAOKM8Dzcfo/mJ9oDD8i98Y\nY5rTF80HezOi9cjyuL41yDwUKF9Zs5ZVKG6xLwqyg1xgezZaoBWGMca0hna3N78TZUDNQSm4m5XW\nIPOQ6ngSVYg/BOzSaIGOYRhjTGtoW3vzMijpvcggLEWptHeV48dRrSYfDuxAD61BrDCMMaY1tFNh\njEDB7c1QCm0AoyOiC2VirUQDlSr3a1iHUcuM2Yv7hcLoL1gJGWNqaZvCyMzFEfFD4O3I9TQGuDgz\np0bEt1F67b0oM2tbqplXa1Drkho/fry/5IwxZiPR7hgGyIhsjdJxby777gT+mJmHR8R/AGfQg8Ko\nbw1ihdF32PgaY2pp94jWp1C/qFHA91HrkF+hmMaBEfEAsBMKjP+10QKtMIwxpjW0W2HMRtXez6K4\nxT4113wedcgFGaxvNlpgOxWGjZMxZiDRboVxNWpG+BvUnLDSCXdL4P+hOo0ngd3K34L6F7PCMMaY\n1tBuhXEc8E9U239Urj0C1WE8iLKpNkOjYisxjucZSDEMG0NjTDtpt8IYjbKjliPXVMWg/BjNwHgN\nSr1dhVqFdKO+cM8YY8zGod0KYxBqa74CqYjflf3HA+8r5z+JguJ/aLTAdg5Q8i9+Y8xAotkApY1d\n6b0X1Ul+g4Df1lwzGPWS2hq4Bw1s6oYHKBljTGtod6X3gVS70QZwZkT8b3m9wahobz7KoDoStUlf\ng75QGFYKxhjTnHZXej+BJu6NQEpiVan0vgN4M2pIOKrcc2mjBTqGYYwxraHdMYw7UUrt25GiGB0R\no4FXUy3YG4qC4o80WmBfKIxWZ1ZZ0Rhj+iNti2GU7deAT6GJe/cAzyFjMh0ZnnvQpL0hwE8aLdAx\nDGOMaQ3tVhi3AZcCU1AdxvIyD+OocnxXZFxmlnM2Sgyj1WzKtSKbOlaHZiDTboXxNJoH3okK835a\n9m9dto8iY7ELPUzcs8IwxpjW0G6FcQTwVVSk9xTVflFDynYCMkA3ogB4N1yHYYwxraGZwjipbDdE\nYdRSrzD+hlqA/AkFtVeU/V8v5+6MZnv/HUqv7YYVhjHGtIZmCuPLwCvYMIVR65KqVxgXIYNxSLnu\namDbzFxVCvvmlfOeA+5rtMB2KgzHIrpj1WXMpkszg9EBRGZOioirUCxhMqqNeBCpidnl3BOBWcAd\nNdevopoO+ywwJTMXRsSoYhAA7gduL/f9EEBEbA0szczhEXEgajr4ZKMFug7DGGNaQzOD8UGAiFiG\n2nQspdqC/EFgEbAjMhx/BkaiFuTXlnM6UT3FXGRMZkdEAnOAFyMjsRmwdzn/mog4uOwbWl43kOtq\nRKMF9scsqVbjX/3GmL6gWQzjAoDMHIom4m2JvrhfDEwEHkIKI4BjkXtpCNWYR0VhzAImAfuWex2V\nmYuB3yMD82cU9P5EZk5FRgZUh3E3Sq+d1WiBjmEYY0xrWFeFsQR9sYNcSROoKowrqCqMweWcisLY\nGVV1z6hRGLshF9dSZIgGAZ+OiAuBf0DZVftQ7TN1CHBL/QIHksKwUjDGtJNmCuNiIIsquAUV1w1H\nBmA8qpeoKIxDUBuPRWggElQVxvblmnHlXgeV40NQPGQJin08gJTLDcgttSvqaNsJHNZogVYYxhjT\nGnrTfDAiYjrKdtoKBag70Jd4IoUB8K3yeAyKb0BVYYCMw00RsRK4F3gLMgoVN9ZwpDoeR7GNOaho\nbxVKsR3WaIGuwzDGmNbQTGFcVLZrS6udzZqsS+HeMOTqehEyLKuRwTkYKZj7qWZjNZQPVhjGGNMa\n+qK9+Y6sybq0Bknk1pqBjMdEpDCGo9jGOKRodqm5Zs0Xq1EYXV1dOdW/+o0xZqPQ7tYgvwJei4Lb\nHchIjCn3GIIK9/ZDrqmG7qb6OoxWFtPZJWWMGUi0bYBS2S5BLT+eRYZiG+SCWl7OWYkMyT5IaXR/\nMSsMY4xpCe1WGEeipoKbledPlnOuRRlSE1GK7krgfxstsJ0KY1PHCsoYU0u7FcZW5fHdSF1sg+o4\nrgEuRIHuQAbmqkYLtMIwxpjW0G6F8TjVYDfA6szMiDgUZV+NROpjAVWjteaLOYZhjDEtod0KY2vk\ncroHGY4JETES9ZbaC7UDCRTjeBVNJu5ZYRhjzMaj3Qpjdbn3i1AL82VIbdxe7rNNOWdH4OU0MBi1\nzJi9eJOOYVjRGGPaSbsVxs/RkKb7UOHe1iiFdr9yn2EoY2oIPdRh1Lqkxo8f7y9VY4zZSLRbYUwF\nXk+1vfmzmbkQuCEiViMDcxewP/DLRgusbw3iGIYxxmwc2q0wtgKuRM0Gr6DM2oiITuD/0HjWMcjQ\n3NVogVYYxhjTGtqtMI4CXgm8HxmlIWX/YOSiWoLag9yC3FR/rX+xdiqM9cVGzRjTH2m3wpiB2qIv\nYc2eVENRkPsKZDAOQG3Tu2GFYYwxraHdCmN3VGexBNgW2Lzs3wtVdq9CxurDwJ8aLbA/KoxWYyNq\njOkLmrU332gKIyI6UDB7Rdm3Wc3rLEIda4egdNtXZebSRgt0e3NjjGkN7VQYI9DMi8Fo6h7Awojo\nQm6oVVTbgtwXEZ2ZuYo6PKLVGGNaQ9sURmYuBm4EFqJCveeArTJzKjJQlW61y1EjwhMaLdAKwxhj\nWkO7Yxg/QtlPnchgVGIYj5bt5qh47xVIiVxc/2L9UWFYKRhj+iNtUxhluxD1kUrWjGFU2oQ8igzO\nIGBaowVaYRhjTGtot8I4D6XQLi7HOmqu+QDwqfJ8MPBwowX2R4XR6kwuKxpjTF/QTGGcVLY9KYyX\nNrhmbV/atVlSV5fHhwLfR2m0FWOzAPgDSrfNss5xjW5ohWGMMa2hmcI4E1VjN1IYgYrrbqm7ppHq\nqPC8wsjMoyPiLuAvaO7FHGRIRqPutb9ESqQTzc04Efh8/Q37o8JYX6wUjDHtpJnCCCAzcxKKNcwF\nJgOHoy/ybYAp5dwHUE3FMGBszT2+DrwZuZ5uRlXbp0bEQ2X/fBTc3hmpia8BS8u9BpVjP0MGpfsC\nrTCMMaYlNFMYCURE3InSWx9BKbCJgtDbo9GpJwNvAJ4o+/eNiLHIBbUPcDZwIPAm5Ma6sdx/OLAD\nKtTbHzUc/ABqODgfmFTOex/wsYYL7AOF4V/uxhjTnGYK43SkMlaijKU9y/5ONOhoWc3zx8vj8cBt\nwEHl+R3A5UAXCm4PR2oD4D3AtUiFvBf4VmY+ATyEqryvzMxA6uXkUh2+BlYYxhjTGpopjM+i0ag9\nZUk1o4NqCm0l7gHVLKl/pNq+fBVSMZ9G3WsnAjtFxLPl+DyqyuN5+kJhrG/WkpWJMWYg0UxhbAl0\nlBjGg8C/Z+akzNwLKYNR6Nf/0yjtdW/gXBSn+Av6kt8bOBYV3c0ATs7Mt5X7T0ezMO4HvggQEROA\nC4FrgI+jud6LkNFaUL9AKwxjjGkNkdnzj/KI+DWaV3EHckstQHMqEniybCehLKeHUOrrCuCmzHxN\nmZo3HRXlTQCeRa6tO4B/QLGMm4AXA6eW49dm5usi4rNIbVR6Sr0jM3+8tjczZOzEHHv8l9ftE2gD\nVibGmBcSETEtM7uanddMYVyAjMr+wPUoLnEA8BbgMOCDKNW1A3gXik8sB+4u1yeKW/wbMhqHZOZk\n4DOZOQH4MUrNPRgZmsXACRGxHXAGUigrkMK4poc3aoVhjDEtoJnBeD9ARCwD3okK6Z4CbgX+Vo7v\njlTAFSgd9llUmV25/2CUMnsnMKPc69pSuHczmn3xIqQivlJmen8aBb23QgHyrYGPNFpgZn4jM7sy\ns6tz2Fbr+v6NMcb0kmYG4xzg2cwcigr0liEVsTNyTb2unNeJJucNRWpg97J/FfBulH67MzCu3Oug\nzDwadaF9sPzNQNXelOfnlXtegtxdOzVaoBWGMca0hmZZUlF3TiXgsRq5mLZB7ilQgPowYDfklgIZ\nkh+h4PlI4NyIOBGgFO4tKPfft7zWLyLiBmQgni7bY5BK6Rbwhv5Z6e1eUsaY/kgzhVFbuDeWauHe\n9ahAby5wFnJFvQF9sU8DJtYV7v0QuBoV7s1E6iGQodiLaq3H9sB3kKH5IHJ7DUJuMHerNcaYNtJM\nYdQX7k0GnkFf6HugeMXuqB3Iw8gA9VS49z7WLNzbBrX8GI7iG69Gg5Wuj4jpwCnl+pVIndzVaIG1\nCqOrqyun+te0McZsFHpjMI6k5+aDW/fiNSpuovrCvflInVyL2oUMRZlWoDjI4nLttkjJfIKq++t5\nIuK9qEqczi23aam7x64eY8xAolkdxn7AbzJzh4g4H5iRmd8uxx5Ggeg5SC3MRupjFDIkuyDVMQgV\n5o1BrqsPZ+YVEbESxTeOArZDKbQjgSuB36M6jRej2o5HgAWZ+fdrezNdXV05derU9fgYjDFm4NLb\nOoxmCmNLZAAaMRUYTTUOUtsuJMvzp5ERqD1W+3gK8ElU77GyHBsMvATFRKDat6rHeRi0SWGsL1Ym\nxpj+SDOFcRXwWhTofohqDKMDBajno6l85wP3lv3DUGD7S0gtLEHZU9dRza66Hvg71LBwF2SUvoP6\nVu0I/Duq8XgUGYrRqEbjQ2t7M1YYxhiz7vSVwjgHOJr1bz64muqUvUbNB78AfLU8fy1q/5ERsRj4\nMirsuwSpj3mNXsAxDGOMaQ3NDMYnqGZJjUNDlHYqz+9FGVKVSu8xaB7GLKrNBwMFs48tz18KLKRq\nMP6GYh87IHUyKCJGoVqOY9FM7yeoZmV1w1lSxhjTGpoZjIuBV2bm/sU9tSuwHzIOM1HK7FvRF/rP\nkeupC7grM+dGRKIK7neUay9Bld8jUXuR04HvosaEmwG/AC5FfaNeh7KlRpS1TG72ZmbMXtwvYhit\nxkrIGNMXNDMYJ6HCvenAFqi30+1U51zchwrqTkFKYzCKcTxWru9AtRqXAaeh2ozbkDq5G7mrtkSF\neX8GdsnME4rKOAcZpYNREP2djRZY65IaP368vxyNMWYj0cxgnInSXnuqw3g5UgW1rC2+UcmoWpWZ\nR0fEq9CI10HIDTUTIDOfiIh5qHV6ltc4Aziu/ob1rUGsMIwxA41W/VDuTS+pzMxJxSW1C3INjUIq\noraX1AOoyG4YaiNSYSjw5rK9GTgRGFl6SV2MajGOoczMiIhLMvM4qtXje6JsrHc0XKAVhjHGtIRm\nabVT0ICje1BDwdoBSktRau1VwMlooNITKFV2X9Sy/FE0z+Js4EAU9H4cDU46ArmwtkXurUXI7TUp\nM/8aEVcCr0HpvH9FrUPGlPbnDekvA5TWFxtDY8zGoK8GKJ1Rzqn0ktqz7K8U0y2ref54eVzbS6oD\nxSouR8Hw2l5SIGPyIMqcWlhe5/6I2BmNdu1AbdH3RdlUleyq2jfq5oPGGNMCWlGHsaLmmvo6jPmo\n/UciF9T5pQ5jLjIgWwCfA/4TeDozF9W/QH9sb76+rG98xsrEGNMXNFMYjeowoFqHUWkbUqnDgO51\nGJOp1mGMQFlUFYOxW7nXatSx9l0RMQF4D8qOmlfWMByNc+2GFYYxxrSGdtdhrEAxkl2QkelAfaWW\nlNev1F6sooE7CvqnwvAvfmNMf6SZwjiZah3GXihAfTtwAzISF6IsJ+i5DmMlqsNYRrUO48LMnIDU\nyJ6oAeHgcs9/Ab6F3FS3o9hFJ1Ig3bDCMMaY1tBMYVwEvIyeYxiNYhn1+2qVQUfdvtuQofkOMixL\nUCbUDUWdrAaeY81YyBr0R4XhWpH+i9WhGcj0RmGAvsyhOgypdrtj3TX1X9qfqXlcaUTYWbZ/Ag5B\nqiTK8ZkR8SaUStuBMqo6qGZorYEVhjHGtIZWKIynax7XK4zjkILYChmlRzJzYUQsojofY0Q5/95G\nC+yPCmN98a9bY0w76QuFUU/9vi1qHj+vMCKiAw1QWgp8BBhCVUXMRUaiEymMlcD0Rgu0wjDGmNbQ\nToUxArUD2QHNxAAYGhEvQ/GKwWjU62rUwLBbDQa0V2H4F78xZiDRNoWRmYtR/GI5MhCJKsGnZebv\nUGX5duX8HqWDFYYxxrSGdscwvoBqNg6KiKeBzWuquX+FOuV2AOdk5pxGC2ynwnC2kzH9B3sENpxm\nCuOkst0QhVFLfQzjMDQnA9QK5Pn1ZOYFaN7GrcDxEbEdDbDCMMaY1rAh8zBA6qOeXs3DAH6DYhi/\nj4jV5VhHRLwsM38bEXtQbR3yDPBGVCi4BgMpS2p98S8rY0xf0ExhPI2GHU1CfaTmonYdhyO1cCSq\nxAbNudgLqHUdrUJtz0GB6ymZORk4NTNfgTrRXomquB9DzQinRcQ41M78GuAP5T6vb7RAKwxjjGkN\nzeZh7IeGHs2k+zyM7VD9xBLUhPAh1KBwBXBTZr6mVGvfilJmJ6Cai0dQhfcRwK+BQ1E21E5InfwA\ntQQ5h2pq7WrUWmSrXMuCN/V5GP0Bqxlj+h99NQ+jAxhUFMYXyvPJSFk8h2ZZnFXOnYO6yt4CHBwR\nY1G32X1QncUc1Kzw88Dp5ZrzUWPCZ9FQpenAaZn5RaRo3g5ch2Z+Pw6MbvBGrTCMMaYFNIthfAoF\nqJehrrOgeALApagF+e7l+WRUhHc/ik8chFTIbZl5TUTcCLwLFevNKtdcgIxPoG62yzPziXJsD9TY\nsBPFMZaYxMwdAAAQuUlEQVQ3WmB/jGH4V7gxpj/STGGcD6zIzKHIRVSZmLcz8EpkcB5AX/hnoqrs\nOWjqHkhhLIuIfZAL6iPISBxUjr8dqY0LkSG6HCAiDivPX4I64z5QXrfbeFYrDGOMaQ3NFMYngM1K\ne/OVKIZxO4phzELqYncUY/gYCnwvQDMz/oLaoY9EA5TuRe6rhVSHLQFcX/aPAG6JiFHl/p2oFmOL\ncs2fGsUv+qPCcP1Gd6y6jHnh00xhXAyQmfujLKbxyPX08rL9G3AM1QFKK9AApfszcy764q8MUHoN\n8FPkejqt5jUeRkFxgA8B56Eg+pPIWHSioPgZjRZohWGMMa2hmcKoHaC0BcqKuh0ZmsoApYOBU2g+\nQOk0qgOU7i0DlCixjUHlvJmZeUJEnI0ypJ5BbrAdgBOoBsufp1ZhdHV15VT/UjXGmI1CWwcoRcTr\nUT+phah31JfK8dFozsbTwIFoNkaP8zCA9wJ0brmN3T0NsLvHGNMX9EZhwIYPUDqyPK5vDfJtZDxG\nAsNQQeC1KKU2kKqZhVJ4Z9MAKwxjjGkN7W5vvlXNsWHAByLiShQ4D1SfMbice12jBVphGNM3WIma\nZvSFwqin1+3NI+IxlEl1DwqGJ0qhfQcyLnPKdhdkuH7S7cWsMIwxpiW0u735g0g9dCJjsRkwEQXQ\no5y3GBmV/ZqslRmzF6+XwvAvK2OMaU7bFEbZXodah+wF/BFVgc8s96jM9B6GDE3DLri1Lqnx48f7\ny98YYzYS7VYY30f1GV3A3wPPZebCiFhac94gVOU9iwbUF+45htFebLCN2XRpt8LYHwW1A9Vn7Fz2\n34Wqw/dCBXyratawBlYYxhjTGtqtMF4BbIkqxLcFNi/7b0bFf50ok6pSxNeNdioMGydjzECi3Qpj\nEaqxABmLB8vjx8t2VDn3WZRJ1Q0rDGOMaQ3tVhgvqltLpTjvOdTO/DFU1Lc5GrzUjYEUw7AxNMa0\nk2bNBze2wvhjzbFngW3K43uAvwM+gIzFIKrzONbAzQeNMaY1NDMYF5XtxlIYS1GMogMZi0ll/2Dg\n98CPkXF5BvWT6kZmfiMzuzKzq3PYVo1OMcYY0we0W2EMQ4V5z6LA910AmbkcVXUPLufel5mraYAV\nhjHGtIZ2xzCWouyoYWXfPgARMRr4NDI+q4GuiDgmM39a/2L9cYDS+tIf4jOOsxiz6dJuhbEtGo70\nWHmNR8v+85Ab6gFgPgqC79BogVYYxhjTGtqtMN6CXFGjkYFYUfYfjnpM7Vbu1wEcj+aBr8FAUhj9\ngf6ggjYEKygzkGm3wvgraji4EGVDfaXsvxxYUo7PB+ahAHg3rDCMMaY1tFthHFzOHws8BbyxvOY4\nNOf7PjRxbwdUj9ENK4zm+FexMaYvaHel9z3A9sD95brhZf/jyGDsAywr5zftVtu55TaNTjHGGNMH\ntFth7F62OyODUan8vgwNUaocX0EPQe+BpDCsFIwx7aTdMYwfozjF/cglVTk+H6mNB8qxwVR7Tq2B\nYxjGGNMa2q0wdkIGpjJtr3LtGait+XjkmlqJUm+70U6F4V/8xpiBRDOFcVLZbojCqKVeYXw0M/cG\nzkUV35XBSUNQmu0IYELZN7rRDa0wjDGmNTRTGF9GMys2RGHUuqTqFca5EfEmpC6eBC4u+69BNRpP\nl/sFmpvRjXYqjPWtObAyMcb0R5oZjA4gMnNSRFwF7AJMRnMqHkRqotKS/EQ0RvWOmutXUW1L/iww\npYxgHVX2XQq8BJiLmhCeUfb/AfgP4B9RPcYUFOPohrOkjDGmNTQzGO8HiIhlVF1Glcl3s4BDUVps\nAn9GtRILgGvLOZ2o9cfccv7siEhgTkTch3pIbQG8GBmey5GR2L28dq06eRdwav0C+2OWlJWJMaY/\n0iyG8SlgRWYOBb6P3EIj0Bf8bsDL0Rd9oBncw6gGqUEK40lkLCYB+5Z7HZWZRwMfBp4AfgGcnpn/\nWK67HjgSZUiB1MVljRboGIYxxrSGyOz5R3lE7Ifma89EE/AWUM1s2g65kZYgF9VDqEJ7BfC7zDy6\nqIlbkRGZgFJjHwFuA45AyuMApHQCqZdfAO8p13UAu5Zjb8vMy9f2ZoaMnZhjj//yOn0ApmesaIwZ\nGETEtMzsanZeM4WxJdCRmZPQQKPLM3NSZu6L4gxzgbOQq2omUhifBKZExFjUA2of4Argq8A04POZ\neUK5/zzgcyi4fS4yFqdl5vKSPXUNcHXlPfXwRq0wjDGmBTRTGFcBrwVuRwpiMlIBHSh2MR+4DjgF\nuLfsH4aMx5eAK5GaOK6ctxplRF2PRrBuTjUD6iFgp8zcomROnYWm8AVSMqdl5lfX9ma6urpy6tSp\n6/YJGGPMAKe3CqNZ0Psc4Gh6TqsFVWP3xGqqLcsr6bFQTau9FzUg7EQuq83L8KTFqBXIwygAvhL4\nVqMXqM+S2tTba68Pdi0ZY/qCZgbjE+hLfiWKT9yDYhgr0Zf97uVvFTAGBbBnobjEX8q1k1FA/C/A\nS1Er84rBuAl1o92cqnIZgdTH4nJ9J7AgMytFfWtQmyXV1dWVU/3laIwxG4VmBuNi4JWZuX9xT+0K\n7IeMw0xUc/FW9KX+c+AwoAu4KzPnlqD3DNRIcFfgEuDdKP32VuA7yK31aVTJ3QF8FgXJV6G25wDb\nRsR/ZWa3tForjIGN1ZMxraM37c0jIqajmoitUDyjAxmJC5FL6RSkNAYjpVDp+9SB1MhlwGnAQSim\ncW9mToiIivqYU+4flYB4RIxHqbz/gLKyvt9ogVYYxhjTGlrRfHBVzeP61iBfRIqkUruxec25PwEm\nlsd/Au5stEArDNNKrGjMQKYvBijtWHdNfdrVZ1ARHnRvPrgMpdYuBbZFCoWI2Bs4pOx/GBX9jaDa\nnLD6YlYYxhjTEtrd3nwS1ULADqrGZo9yTgewqNzjJfRQ7V1hxuzFVhgN8K9iY0xf0O4RrdujrKm5\nSKmMiYgOlDnViQLrm6GA+Z6NFljrkho/fry/HI0xZiPRboXRWbOvUq8xseb8cTX33KnRAuubD7ZS\nYdg4GWMGEs0MRrMBSi9tcE2vBihFxNWow+0wZDgGlePzUGrtG4HzgI+jDKzDG93QCsMYY1pDM4Nx\nJnAUjRVGoG61t9Rd07DnU+F5hVGaE+4NfBO1QN8aeCIzFwFExLkobfcxpD6+1OiG7VQYmzo2vsaY\nWpoZjACyhwFKj6FeT1PKuQ+gTKdhVAvuAL6OekUNRZ1vTwRGRsRDwLfLsbGUYU0RcQnwL1Qn7o1D\n7c3/0HCBVhjGGNMSmhmMRF/id6L25o+gwr1EnWe3B65CwfE3oNYg04B9S7fa1ahb7dkokP0m5Ma6\nsdx/ATIYWwHTy/PTUDHgK1GsY3U5Z2/g7m4LHEAKw8bQGNNOmrU3P4NqtfajVDOVOlFwelnN88fL\n4/Gomvugcu3daJJeF+oPNRypDVBB3mpkgEYDozPziXIuyGDch2Id/x0RI+sX6PbmxhjTGvqiW+3a\naNat9nRU5b0Cpc9uX/YfUs6dj4zOoah9yETqYib9cUSrlYIxpj/STGE06lYL1W61o8rzSrdaWHu3\n2hGo11TFYOyFDEUgl9fQiBiF1MyTqIZjZTk+GtVlrIEVhjHGtIZmA5SOBS7LzI61dKvdAxmOb6Ju\ntbujbrUHRsQq1JV2JI271X4W+CiafTESGYbvoUl7RyDjcgxyc12Ume9f25vxiNbGWNEYY9ZGX41o\nre1WuxfKgroduIFqt9oflXObdatdRrVb7YWZOQEZm++gGd6BYhnvRgbj1Sj4vWPZf1EPb9QKwxhj\nWkC7u9W+D2VWjQWeK39jMnN+RCxHBmMV8N/A/Y0W6BiGMca0ht4oDNjwbrUV6ntJ3YHcWUvLvueA\nBWVOxjhgSdk/raeJe1YYxhjTGtrdS2oIKvQLlCn1QGZmRLwPpd5W7vXNiPhbZt5U/2L9UWH0l1oR\nKyFjTC19oTDq6VW32tKV9oCyhpnIiEwux58t9xkMTEWZV29rtEArDGOMaQ3tVBgjqE7YqwTMiYiX\nUS0ErGwfRbMzutEfFUZ/ob8ooU0ZqzzzQqKZwmjWrbY3CqOW5xVGZi4GfgDcVf4WILfUNNSCZBXK\nmAKl5M5rdEMrDGOMaQ0b0q22dltLr7rVlu2/Av+LmhhuDSzJzEUR8SgyHOeVY48BP2t0w75QGP4V\nZ4wxzWmmMJ5GrcgnoSrvuSjOcDhSHQnMLueeiGo15tRcvwpVbIPiElMyczJwatl3EvC9zBwH/BBY\nXWIb16CmhacB/4eqwa9utEArDGOMaQ3NFMYiYGkP3WpvLcdBxuO/UCxiDt1rJuYCfwRuKtXftwEn\noLbkK0uL8t+hNuaVOozPo/qLkcCppSlhN2oVRldXV061WjDGmI1CM4XxFLC4KIwvlPMnA0cCO6P+\nUA+U/acihfEc1dkVj6LCvOtRp9uXFIVxejl+J/C5zNwDDUjaHMUyyMxvA28HbsjM72zwOzXGGLNB\nrFVhZObCiPhjURi/RoV2FYXxr5k5NyJASqOiMG4Afl53n7si4iy6K4yPoBqLD5d7npCluVVE/B4Z\nmS1KTOPEzLymb962McaYdWWtzQd7dYOII4CPZuZr+2RFG0BXV1dOnTq13cswxph+RV81HzTGGGOA\n5kHvpmTmjVRHrhpjjNlEscIwxhjTK2wwjDHG9AobDGOMMb3CBsMYY0yv2OC02hcSEfEUcF+71/EC\nZAzq+muq+DPpjj+TxgyEz2XnzNym2UkbnCX1AuO+3uQSDzQiYqo/lzXxZ9IdfyaN8edSxS4pY4wx\nvcIGwxhjTK/Y1AzGN9q9gBco/ly648+kO/5MGuPPpbBJBb2NMcZsPDY1hWGMMWYjsckYjIh4VUTc\nFxEPRMTH272eFwIR8VBEzIiI6RExYNv4RsS3I2J+adNf2TcqIq6NiPvLdut2rrHV9PCZfDYiZpf/\nl+kRcXQ719hqImKniLghIu6OiLsi4rSyf0D/r9SySRiMiOgEvga8GtgbeGtE7N3eVb1geGlm7j/A\n0wK/C7yqbt/Hgd9m5kTgt+X5QOK7dP9MAM4v/y/7Z2bDscibMCuBj2Tm3sAhwCnle2Sg/688zyZh\nMICDgQcyc2ZmPgf8GHh9m9dkXiBk5u/Q5MdaXg98rzz+HvBPLV1Um+nhMxnQZOZjmXlrefwUcA+w\nIwP8f6WWTcVg7IjmjVd4tOwb6CRwXURMK3PTTZXtMvOx8ngusF07F/MC4oMRcUdxWQ1Y10tETAAO\nAP6M/1eeZ1MxGKYxUzJzf+SqOyUiDm/3gl6IlLHATheEC4Fdgf2Bx4AvtXc57SEitgB+CnwoM5fU\nHhvo/yubisGYDexU83xc2TegyczZZTsfzVk/uL0rekExLyLGApTt/Davp+1k5rzMXJWZq4FvMgD/\nXyJiMDIWP8zMn5Xd/l8pbCoG4xZgYkTsEhGbAf8M/E+b19RWImJ4RIyoPAZeAdy59qsGFP8DHF8e\nHw9c2ca1vCCofCkW3sAA+3+JiAAuBu7JzPNqDvl/pbDJFO6VFMAvA53AtzPzrDYvqa1ExK5IVYCa\nTF46UD+TiPgRcATqOjoP+AzwC+AyYDwwC3hzZg6YIHAPn8kRyB2VwEPA+2p895s8ETEF+D0wA1hd\ndv8bimMM2P+VWjYZg2GMMWbjsqm4pIwxxmxkbDCMMcb0ChsMY4wxvcIGwxhjTK+wwTDGGNMrbDCM\nMcb0ChsMY4wxvcIGwxhjTK/4/ykiStIkTN9dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112593ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens = [len(value) for value in test_topic2docs.values()]\n",
    "keys = len(test_topic2docs.keys())\n",
    "plt.barh([i for i in range(keys)], lens, tick_label = test_topic2docs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing an answer given texts from the same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a method to choose answer from the original text AND from texts from the same cluster\n",
    "# answer as dictionary, original text as list of strings, similar_texts as one list of strings\n",
    "def choose_answer_v2(original_text, similar_texts, answers):\n",
    "    original_answer = choose_answer(original_text, answers)\n",
    "    #if we don't find the answer in the original text\n",
    "    if original_answer[1] == 0:\n",
    "        similar_answer = choose_answer(similar_texts, answers)\n",
    "        return similar_answer\n",
    "    else:\n",
    "        return original_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_by_text_id(text_id, topic2docs):\n",
    "    for topic in topic2docs.keys():\n",
    "        for text in topic2docs[topic]:\n",
    "            if text['text_id'] == text_id:\n",
    "                return topic\n",
    "\n",
    "def get_topic_texts(topic, topic2docs):\n",
    "    texts_list = topic2docs[topic]\n",
    "    texts = []\n",
    "    for text in texts_list:\n",
    "        texts += text['text']\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v2(data, topic2docs, choose_answer_function):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        instance_topic = get_topic_by_text_id(instance['text_id'], topic2docs)\n",
    "        similar_texts = get_topic_texts(instance_topic, topic2docs)\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer_function(instance['text'], similar_texts, question['answers'])[0]\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigARTM train accuracy: 61.59695817490495\n",
      "BigARTM dev accuracy: 61.51665485471297\n",
      "BigARTM test accuracy: 61.315695387915625\n"
     ]
    }
   ],
   "source": [
    "train_with_bigARTM = run_v2(train_data, train_topic2docs, choose_answer_v2)\n",
    "print(\"BigARTM train accuracy: \" + str(evaluate(train_with_bigARTM, gold_train)[0]))\n",
    "dev_with_bigARTM = run_v2(dev_data, dev_topic2docs, choose_answer_v2)\n",
    "print(\"BigARTM dev accuracy: \" + str(evaluate(dev_with_bigARTM, gold_dev)[0]))\n",
    "test_with_bigARTM = run_v2(test_data, test_topic2docs, choose_answer_v2)\n",
    "print(\"BigARTM test accuracy: \" + str(evaluate(test_with_bigARTM, gold_test)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 incorrent answers:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['0', '5', '0'],\n",
       " ['1', '1', '0'],\n",
       " ['1', '3', '1'],\n",
       " ['1', '4', '1'],\n",
       " ['1', '7', '0'],\n",
       " ['2', '0', '1'],\n",
       " ['3', '0', '0'],\n",
       " ['4', '0', '1'],\n",
       " ['4', '1', '0'],\n",
       " ['4', '3', '1']]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First 10 incorrent answers:\")\n",
    "tup_with_bigARTM[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Answering Using Related Script Knowledge\n",
    "terminology: _topic-script_ is a topic section from DeScript, e.g. baking a cake, flying in an airplane\n",
    "- mapping text to topic-script\n",
    "- finding answer in the topic-script\n",
    "\n",
    "_IDEA_: doc similarity between topic-script and given text with Doc2Vec<br>\n",
    "_IDEA_: map text to a specific event from the scripts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Similarity Between Given Text and Topic-Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper method - from tuples to dictionary\n",
    "def tuple_to_dict(some_list):\n",
    "    new_list = {}\n",
    "    for pair in some_list:\n",
    "        new_list[pair[0]] = pair[1]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding frequencies for topics and scaling them by number of documents\n",
    "topic_vectors = {}\n",
    "for topic in topics.keys():\n",
    "    topic_vectors[topic] = {}\n",
    "    meaningful_words = list(filter(lambda x: x not in stopwords, topics[topic]))\n",
    "    words = tuple_to_dict(nltk.FreqDist(meaningful_words).most_common(12))\n",
    "    \n",
    "    current_docs = topic_docs[topic]\n",
    "    for word in words:\n",
    "        number_of_docs = 0\n",
    "        for doc in current_docs:\n",
    "            if word in doc:\n",
    "                number_of_docs += 1\n",
    "        topic_vectors[topic][word] = words[word]/number_of_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting all texts and cleaning\n",
    "clean_all_texts = []\n",
    "for text in all_texts:\n",
    "    ind = text[0].index(\"|\")\n",
    "    clean_all_texts.append([text[0][ind+6:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting frequencies for words in all documents\n",
    "full_texts = []\n",
    "full_texts = np.append(full_texts, clean_all_texts)\n",
    "just_text = \" \".join(full_texts)\n",
    "words = nltk.tokenize.word_tokenize(just_text)\n",
    "fd = nltk.FreqDist(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleaning and finding frequencies for given text\n",
    "def get_text_vector(text):\n",
    "    meaningful_words = list(filter(lambda x: x not in stopwords, text))\n",
    "    text_vector = tuple_to_dict(nltk.FreqDist(meaningful_words).most_common(12))\n",
    "    for word, freq in text_vector.items():\n",
    "        if word in fd:\n",
    "            text_vector[word] = freq/(fd[word])\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# cosine similarity function to compare two vectors with frequencies\n",
    "def cosine_similarity(topic_vector, text_vector):\n",
    "    \n",
    "    topic_sum = 0\n",
    "    text_sum = 0\n",
    "    dot_product = 0\n",
    "    \n",
    "    for word in topic_vector:\n",
    "        topic_sum += (topic_vector[word]**2)\n",
    "        if word in text_vector.keys():\n",
    "            text_sum += (text_vector[word]**2)\n",
    "            dot_product += (topic_vector[word] * text_vector[word])\n",
    "    topic_norm = math.sqrt(topic_sum)\n",
    "    text_norm = math.sqrt(text_sum)\n",
    "    \n",
    "    if topic_norm * text_norm == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product/(topic_norm * text_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns most similar topic\n",
    "def get_topic(text, topic_vectors):\n",
    "    topic_scores = {}\n",
    "    for topic in topic_vectors.keys():\n",
    "        csim = cosine_similarity(topic_vectors[topic], get_text_vector(text))\n",
    "        topic_scores[topic] = csim\n",
    "    return max(topic_scores.items(), key=lambda k: k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering Using Topic-Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v3(data, topic_vectors, topics, choose_answer_function):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            current_topic = get_topic(instance['text'], topic_vectors)[0]\n",
    "            correct_answer = choose_answer_function(instance['text'], topics[current_topic], question['answers'])[0]\n",
    "            results.append([instance['text_id'], question['question_id'], correct_answer])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_with_scripts = run_v3(train_data, topic_vectors, topics, choose_answer_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using topic-scripts:\n",
      "60.41516801973076\n"
     ]
    }
   ],
   "source": [
    "tup_scripts = evaluate(res_with_scripts, ideal_results_train)\n",
    "print(\"Accuracy using topic-scripts:\")\n",
    "print(tup_scripts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.67399007795889"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_res_scripts = run_v3(dev_data, topic_vectors, topics, choose_answer_v2)\n",
    "evaluate(dev_res_scripts, ideal_results_dev)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Clustering with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- part 1: embed texts with Doc2Vec\n",
    "- part 2: cluster embedded representations with KMeans (or other clustering methods)\n",
    "- part 3: cluster - texts mapping\n",
    "- part 4: choose answer, using a given text and texts from the same cluster\n",
    "\n",
    "_Inspired by_: https://towardsdatascience.com/automatic-topic-clustering-using-doc2vec-e1cea88449c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Doc2Vec and Inferring Vectors for all Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sonya/anaconda/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for t in clean_all_texts:\n",
    "    data.append(nltk.tokenize.word_tokenize(t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additional_texts_from_descript = []\n",
    "for topic in topics:\n",
    "    l = len(topics[topic])\n",
    "    data.append(topics[topic][:int(l/3)])\n",
    "    data.append(topics[topic][int(l/3):int(l-(l/3))])\n",
    "    data.append(topics[topic][int(l-(l/3)):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Documents(object):\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            yield TaggedDocument(words = doc, tags = [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = Documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "count = len(data)\n",
    "model = Doc2Vec(size=100, dbow_words= 1, dm=0, iter=1,  window=10, seed=1337, min_count=5, workers=4,alpha=0.025, min_alpha=0.025)\n",
    "model.build_vocab(documents)\n",
    "for epoch in range(20):\n",
    "    print(\"epoch \"+str(epoch))\n",
    "    model.train(documents, total_examples=count, epochs=1)\n",
    "    model.save('commonSense.model')\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inferring vectors\n",
    "vectors = []\n",
    "for d in data:\n",
    "    vectors.append(model.infer_vector(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclusterer = KMeansClusterer(T, distance=nltk.cluster.util.cosine_distance, repeats=20)\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEFZJREFUeJzt3V2sXFd9hvHnxUD4Bqeh1pET10GykFJoobIoKlykpLQp\nIJwrK0hILk3lG4qgakUcblArRfJFhcIFvbBCiiso4PLRRBSppIEIetHAMUkVSLCIICF27Tg0IdCC\noAn/XsyGjIx9PmZmz+w56/lJ1szs+Vpacd7zrjX7jFNVSJK2tmcsegCSpP4Z9pLUAMNekhpg2EtS\nAwx7SWqAYS9JDTDsJakBhr0kNcCwl6QGPHPRAwC45JJLavfu3YsehiQtlePHj3+/ql66kccOIux3\n797N6urqoochSUslyUMbfazbOJLUAMNekhpg2EtSA9YN+yS3JDmb5Btjxy5OcnuSb3eX28fuuyHJ\nA0lOJPmjvgYuSdq4jTT7jwBXn3PsEHBHVe0B7uhuk+QK4FrgN7vn/F2SbTMbrSRpIuuGfVV9GXjs\nnMP7gKPd9aPANWPHP1FVP62q7wIPAK+Z0VglSROadM9+R1Wd7q6fAXZ013cCD4897mR3TJK0QFN/\nQFujf9dw0/+2YZKDSVaTrD766KPTDkOStIZJf6nqkSQrVXU6yQpwtjt+Crhs7HGXdsd+RVUdAY4A\nXLSyp3Yf+pcJh6Khe/Dwmxc9BKl5kzb724AD3fUDwK1jx69NclGSy4E9wFenG6IkaVrrNvskHweu\nBC5JchJ4P3AYOJbkOuAhYD9AVX0zyTHgPuBJ4J1V9VRPY1dPbOLS1rNu2FfV2y5w11UXePyNwI3T\nDEqSNFuD+CI0DcvQPz9x5SFtnl+XIEkNsNlvgE1S0rKz2UtSA2z2GzDJHrarAUlDYrOXpAbY7Kdk\ng5e0DGz2ktQAm/2U3M+XtAxs9pLUgOaava1aUots9pLUgOaa/dC/96U1rrSk+bDZS1IDmmv2y8oG\nLGkaNntJaoDNfko2bknLwGYvSQ1ortnbxCW1yGYvSQ1ortl7nv38uZqSFs9mL0kNaK7Za302cWnr\nsdlLUgNs9mNstJK2Kpu9JDXAZj+mpTN1XMVIbbHZS1IDbPaNGsoqxhWGNB82e0lqgM1+jC1T0lY1\nVdgn+Qvgz4AC7gXeATwP+CSwG3gQ2F9Vj6/1Oq/c+WJWDVpJ6k2qarInJjuBfweuqKqfJDkGfB64\nAnisqg4nOQRsr6rr13qti1b21MqBmyYax6K5GpC0KEmOV9XejTx22m2cZwLPTfJ/jBr9fwE3AFd2\n9x8F7gTWDHubvST1a+Kwr6pTSf4W+B7wE+ALVfWFJDuq6nT3sDPAjvVe695TTwzm7BBNxhWONGwT\nh32S7cA+4HLgB8A/JXn7+GOqqpKcd58oyUHgIMCuXbsMC0nq0TTbOH8AfLeqHgVI8hng94BHkqxU\n1ekkK8DZ8z25qo4AR2C0Z2+z1zh/+EuzNU3Yfw94bZLnMdrGuQpYBf4XOAAc7i5vXe+F3LOXpH5N\ns2d/V5JPAV8HngTuZtTUXwAcS3Id8BCwf73Xcs9+/mzOUlsmPvVylvbu3Vurq6uLHoYkLZV5nno5\nEzb75edKQRq2QYS9e/aS1K9BhP3Qm72tVdKyG0TY2+wlqV+DCPuhN3sNiystafMGEfY2e0nq1yDC\n3mYvaT2u6KYziLC32UtSvwYR9jZ7DZFNUlvJIMLeZi9J/RpE2NvspflxxdKmQYS9zV6S+jWIsLfZ\nt8uWKc3HIMLeZi9J/RpE2Nvs22Wzl+ZjEGFvs5ekfg0i7G32y8+GLg3bIMLeZi9J/RpE2NvstzZb\nv7R4gwh7m70k9WsQYW+z39ps9tLiDSLsbfaS1K9BhP08m70tU1KLBhH2NntJ6tcgwt49e53LFZg0\nW4MIe5u9JPVrEGFvsx8WW7W09Qwi7G32ktSvQYS9zb5driKk+RhE2NvsJalfgwh7m73OZeOXZmuq\nsE/yEuBm4BVAAX8KnAA+CewGHgT2V9Xja72OzV6S+pWqmvzJyVHgK1V1c5JnA88D3gc8VlWHkxwC\ntlfV9Wu9zkUre2rlwE0Tj0PLywYvTS7J8arau6HHThr2SV4M3AO8rMZeJMkJ4MqqOp1kBbizql6+\n1msZ9poFf3CoNZsJ+2dM8T6XA48Cf5/k7iQ3J3k+sKOqTnePOQPsuMAgDyZZTbL61I+fmGIYkqT1\nTNPs9wL/Abyuqu5K8kHgh8C7quolY497vKq2r/VaNntpflwBbR3zavYngZNVdVd3+1PA7wCPdNs3\ndJdnp3gPSdIMTHw2TlWdSfJwkpdX1QngKuC+7s8B4HB3eetMRqqlZZOUFm/a8+zfBXysOxPnO8A7\nGK0WjiW5DngI2D/le0iSpjRV2FfVPcD59ouumuZ1tbWs9Qtztn5pPqbZs5ckLYlBfF2C2uXXZGij\nXAVOx2YvSQ2w2S8JW42kadjsJakBNvsl4d62FsmV5fKz2UtSA2z2jbKpSW2x2UtSA2z2jfIzgPlz\nNaVFstlLUgNs9lo6NmRp82z2ktQAm72WzoU+b7DxSxdms5ekBtjsNUi2dGm2bPaS1ACbvQbJ3wPQ\nVjKElarNXpIaYLPXrxhCC5E0WzZ7SWqAzV6/Yp775a4ipPmw2UtSA2z2U7KZSloGNntJaoDNfkqe\nD751uWrTVmKzl6QG2OzVOxuytHg2e0lqgM1evfNzjfW5+lHfpm72SbYluTvJ57rbFye5Pcm3u8vt\n0w9TkjSNWTT7dwP3Ay/qbh8C7qiqw0kOdbevn8H7aEnZWqXFm6rZJ7kUeDNw89jhfcDR7vpR4Jpp\n3kOSNL1pm/1NwHuBF44d21FVp7vrZ4AdU76Hlpx79tooV4H9mbjZJ3kLcLaqjl/oMVVVQF3g+QeT\nrCZZferHT0w6DEnSBkzT7F8HvDXJm4DnAC9K8lHgkSQrVXU6yQpw9nxPrqojwBGAi1b2nPcHgqT5\nsVVvbRM3+6q6oaourardwLXAF6vq7cBtwIHuYQeAW6cepSRpKn2cZ38YOJbkOuAhYH8P7yFpxvxs\nZTpDXxnNJOyr6k7gzu76fwNXzeJ1JUmz4W/QbnFDbxuS5sPvxpGkBtjst7h57cO6gpCGzWYvSQ2w\n2WsmPJPjaa5yNEQ2e0lqgM1emjFXOdqoea4CbfaS1ACbvTbF/WhpOdnsJakBNnttivvRy8/VWZts\n9pLUAJu9Bsn2Kc2WzV6SGmCz1yD52YA2w5Xg+gYR9q/c+WJW/Y8lSb0ZRNjfe+oJm5w2zBYnbd4g\nwt5mL0n9GkTY2+y3Npu4tHiDCHubvST1axBhb7PXLLiCkC5sEGFvs5ekfg0i7Ife7G2MkpbdIMLe\nZi9J/RpE2A+92as/rpqk+RhE2NvsJalfgwh7m700bK7Alt8gwt5mL0n9GkTY2+wlgSuIPg0i7G32\nktSvQYT90Ju9bUPSsps47JNcBvwDsAMo4EhVfTDJxcAngd3Ag8D+qnp8rdey2UtSv1JVkz0xWQFW\nqurrSV4IHAeuAf4EeKyqDic5BGyvquvXeq2LVvbUyoGbJhqHhsHVjzR/SY5X1d6NPHbiZl9Vp4HT\n3fUfJbkf2AnsA67sHnYUuBNYM+xt9pLUr5ns2SfZDbwauAvY0f0gADjDaJtnTUPfs5f65spIfZs6\n7JO8APg08J6q+mGSX95XVZXkvPtESQ4CBwF27drlX3ZJ6tFUYZ/kWYyC/mNV9Znu8CNJVqrqdLev\nf/Z8z62qI8ARGO3Z2+y1VVhcNETTnI0T4MPA/VX1gbG7bgMOAIe7y1vXey337CWpX9OcjfN64CvA\nvcDPu8PvY7RvfwzYBTzE6NTLx9Z6Lc/G0SzYqNWazZyNM3HYz9LevXtrdXV10cOQpKUyl1MvZ8mz\ncbQZNnhp8wYR9u7ZS1K/BhH2NnsNkSsIbSWDCHubvST1axBhb7OXLswVhmZhEGFvs5ekfg0i7G32\naoENXYs0iLC32UtSvwYR9jZ7tcBmr0UaRNjb7CWpX4MIe5u9ND+uMNo0iLC32UtSvwYR9jZ7SUOy\nFVc/gwh7m70k9WsQYW+zb9dWbFDSEA0i7G32ktSvQYS9zV6aH1dTbRpE2Euan1kWK39wLI9nLHoA\nkqT+2ey1UDZDaT5s9pLUAJu9FsoP5jULrhDXZ7OXpAbY7DVINjVptmz2ktQAm70WygYvzYfNXpIa\nYLNvlI1aaovNXpIaYLNv1Frnt9v6pa2nt2af5OokJ5I8kORQX+8jSVpfL80+yTbgQ8AbgZPA15Lc\nVlX39fF+mi1/q3XEFY62kr6a/WuAB6rqO1X1M+ATwL6e3kuStI6+9ux3Ag+P3T4J/G5P76UtyFYt\nzdbCzsZJcjDJapLVp378xKKGIUlN6KvZnwIuG7t9aXfsl6rqCHAEYO/eveW/QStJ/emr2X8N2JPk\n8iTPBq4FbuvpvSRJ6+il2VfVk0n+HPhXYBtwS1V9s4/3kiStr7dfqqqqzwOf7+v1JUkb59clSFID\nDHtJaoBhL0kNMOwlqQGGvSQ1wLCXpAakqhY9BpL8CDix6HEMxCXA9xc9iAFwHp7mXIw4DyPj8/Ab\nVfXSjTxpKP94yYmq2rvoQQxBklXnwnkY51yMOA8jk86D2ziS1ADDXpIaMJSwP7LoAQyIczHiPDzN\nuRhxHkYmmodBfEArSerXUJq9JKlHCw/7JFcnOZHkgSSHFj2eeUpyS5KzSb4xduziJLcn+XZ3uX2R\nY5yHJJcl+VKS+5J8M8m7u+NNzUWS5yT5apL/7Obhr7vjTc3DLyTZluTuJJ/rbrc6Dw8muTfJPUlW\nu2ObnouFhn2SbcCHgD8GrgDeluSKRY5pzj4CXH3OsUPAHVW1B7iju73VPQn8ZVVdAbwWeGf396C1\nufgp8Iaq+m3gVcDVSV5Le/PwC+8G7h+73eo8APx+Vb1q7JTLTc/Fopv9a4AHquo7VfUz4BPAvgWP\naW6q6svAY+cc3gcc7a4fBa6Z66AWoKpOV9XXu+s/YvQ/+E4am4sa+Z/u5rO6P0Vj8wCQ5FLgzcDN\nY4ebm4c1bHouFh32O4GHx26f7I61bEdVne6unwF2LHIw85ZkN/Bq4C4anItu6+Ie4Cxwe1U1OQ/A\nTcB7gZ+PHWtxHmD0A//fkhxPcrA7tum5GMpv0Oo8qqqSNHO6VJIXAJ8G3lNVP0zyy/tamYuqegp4\nVZKXAJ9N8opz7t/y85DkLcDZqjqe5MrzPaaFeRjz+qo6leTXgduTfGv8zo3OxaKb/SngsrHbl3bH\nWvZIkhWA7vLsgsczF0mexSjoP1ZVn+kONzkXAFX1A+BLjD7TaW0eXge8NcmDjLZ235Dko7Q3DwBU\n1anu8izwWUbb35uei0WH/deAPUkuT/Js4FrgtgWPadFuAw501w8Aty5wLHORUYX/MHB/VX1g7K6m\n5iLJS7tGT5LnAm8EvkVj81BVN1TVpVW1m1EmfLGq3k5j8wCQ5PlJXviL68AfAt9ggrlY+C9VJXkT\no/25bcAtVXXjQgc0R0k+DlzJ6FvsHgHeD/wzcAzYBTwE7K+qcz/E3VKSvB74CnAvT+/Rvo/Rvn0z\nc5Hktxh92LaNURE7VlV/k+TXaGgexnXbOH9VVW9pcR6SvIxRm4fRtvs/VtWNk8zFwsNektS/RW/j\nSJLmwLCXpAYY9pLUAMNekhpg2EtSAwx7SWqAYS9JDTDsJakB/w/33nQEcmrRtQAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119dab550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of clusters per all 2119 texts (from train, dev and test combined)\n",
    "cnt = Counter(assigned_clusters)\n",
    "plt.barh([i for i in range(len(cnt.keys()))], [value for value in cnt.values()])\n",
    "plt.savefig(\"fig_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text2Cluster/Cluster2Texts Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from assigned clusters to actual text_ids from all_texts\n",
    "mapping_text2cluster = {}\n",
    "for i in range(len(all_texts)):\n",
    "    cur_ind = all_texts[i][0].index(\"|\")\n",
    "    text_id = all_texts[i][0][:cur_ind-1]\n",
    "    mapping_text2cluster[text_id] = assigned_clusters[i]\n",
    "mapping_text2cluster['test429']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapping from cluster number to text_ids\n",
    "mapping_cluster2texts = {}\n",
    "for key,value in list(mapping_text2cluster.items()):\n",
    "    if value not in mapping_cluster2texts:\n",
    "        mapping_cluster2texts[value] = []\n",
    "        mapping_cluster2texts[value].append(key)\n",
    "    else:\n",
    "        mapping_cluster2texts[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(75, 46), (38, 47), (6, 48)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most frequent clusters\n",
    "map_cluster_len = {}\n",
    "for key, value in mapping_cluster2texts.items():\n",
    "    map_cluster_len[key] = len(value)\n",
    "\n",
    "import operator\n",
    "sorted_len = sorted(map_cluster_len.items(), key=operator.itemgetter(1))\n",
    "sorted_len[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_from_mapping_id(mapping_id):\n",
    "    if 'train' in mapping_id:\n",
    "        text_id = mapping_id[5:]\n",
    "        return get_text_by_id(text_id, train_data)['text']\n",
    "    elif 'dev' in mapping_id:\n",
    "        text_id = mapping_id[3:]\n",
    "        return get_text_by_id(text_id, dev_data)['text']\n",
    "    elif 'test' in mapping_id:\n",
    "        text_id = mapping_id[4:]\n",
    "        return get_text_by_id(text_id, test_data)['text']\n",
    "    \n",
    "# returns list of lists (with texts divided into words)\n",
    "def get_all_texts_per_cluster(cluster_number, mapping_cluster2texts):\n",
    "    current_mapping_ids = mapping_cluster2texts[cluster_number]\n",
    "    texts_per_cluster = []\n",
    "    for map_id in current_mapping_ids:\n",
    "        texts_per_cluster.append(get_text_from_mapping_id(map_id))\n",
    "    return texts_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO BE MODIFIED TO GET CLEANED TEXTS\n",
    "def get_text_from_mapping_id(mapping_id):\n",
    "    if 'train' in mapping_id:\n",
    "        text_id = mapping_id[5:]\n",
    "        return get_text_by_id(text_id, train_data)['text']\n",
    "    elif 'dev' in mapping_id:\n",
    "        text_id = mapping_id[3:]\n",
    "        return get_text_by_id(text_id, dev_data)['text']\n",
    "    elif 'test' in mapping_id:\n",
    "        text_id = mapping_id[4:]\n",
    "        return get_text_by_id(text_id, test_data)['text']\n",
    "    \n",
    "# returns list of lists (with texts divided into words)\n",
    "def get_all_texts_per_cluster(cluster_number, mapping_cluster2texts):\n",
    "    current_mapping_ids = mapping_cluster2texts[cluster_number]\n",
    "    texts_per_cluster = []\n",
    "    for map_id in current_mapping_ids:\n",
    "        texts_per_cluster.append(get_text_from_mapping_id(map_id))\n",
    "    return texts_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 6: [('the', 703), ('i', 392), ('to', 325), ('and', 300), ('a', 226), ('it', 204), ('wall', 204), ('we', 156), ('paint', 150), ('of', 142), ('my', 128), ('in', 124), ('on', 119), ('be', 106), ('that', 94)]\n",
      "cluster 38: [('the', 760), ('i', 465), ('and', 321), ('bed', 261), ('sheet', 230), ('to', 225), ('of', 169), ('my', 160), ('on', 143), ('it', 142), ('pillow', 107), ('in', 97), ('put', 95), ('a', 91), ('them', 86)]\n",
      "cluster 75: [('the', 759), ('i', 446), ('and', 357), ('to', 203), ('dish', 199), ('in', 167), ('of', 148), ('a', 135), ('them', 123), ('be', 105), ('it', 104), ('put', 101), ('dishwasher', 90), ('with', 73), ('have', 73)]\n"
     ]
    }
   ],
   "source": [
    "# to find out top 15 tokens for 3 most frequent clusters\n",
    "print(\"cluster 6: \" + str(nltk.FreqDist(np.concatenate(get_all_texts_per_cluster(6, mapping_cluster2texts))).most_common(15)))\n",
    "print(\"cluster 38: \"  + str(nltk.FreqDist(np.concatenate(get_all_texts_per_cluster(38, mapping_cluster2texts))).most_common(15)))\n",
    "print(\"cluster 75: \" + str(nltk.FreqDist(np.concatenate(get_all_texts_per_cluster(75, mapping_cluster2texts))).most_common(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing an Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v5(data, mapping_cluster2texts, mapping_text2cluster, datatype_prefix):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        cluster_number = mapping_text2cluster[datatype_prefix + instance['text_id']]\n",
    "        similar_texts = np.concatenate(get_all_texts_per_cluster(cluster_number, mapping_cluster2texts))\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer_v2(instance['text'], similar_texts, question['answers'])[0]\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Doc2Vec/KMeans with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.4633645051896"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_res_train = run_v5(train_data, mapping_cluster2texts, mapping_text2cluster, 'train')\n",
    "evaluate(gensim_res_train, ideal_results_train)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Doc2Vec/KMeans with dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.09142452161588"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_res_dev = run_v5(dev_data, mapping_cluster2texts, mapping_text2cluster, 'dev')\n",
    "evaluate(gensim_res_dev, ideal_results_dev)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim_res_test = run_v5(test_data, mapping_cluster2texts, mapping_text2cluster, 'test')\n",
    "save_results(gensim_res_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5. Choosing an Answer Based on Similarity\n",
    "- train doc2vec with sentences from texts, questions and answers\n",
    "- infer vectors for Q&As\n",
    "- score answers based on their similarity with the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_preprocessing(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = []\n",
    "    for sent in sentences:\n",
    "        tokenized_sentences.append(preprocess_text(sent))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml2listData_v2(xml_data):\n",
    "    data = []\n",
    "    root = ET.XML(xml_data)\n",
    "    for child in root:\n",
    "        for subchild in child:\n",
    "            if subchild.tag == 'text':\n",
    "                data.append(sentence_preprocessing(subchild.text))\n",
    "            if subchild.tag == 'questions':\n",
    "                for question in subchild:\n",
    "                    data.append(sentence_preprocessing(question.attrib[\"text\"]))\n",
    "                    for answer in question:\n",
    "                        data.append(sentence_preprocessing(answer.attrib[\"text\"]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_train = xml2listData_v2(xml_train)\n",
    "new_dev = xml2listData_v2(xml_dev)\n",
    "new_test = xml2listData_v2(xml_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for el in np.concatenate([new_train, new_dev, new_test]):\n",
    "    for s in el:\n",
    "        all_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Documents(object):\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            yield TaggedDocument(words = doc, tags = [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_sentences = Documents(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "count = len(all_sentences)\n",
    "model_sim = Doc2Vec(size=40, dbow_words= 1, dm=0, iter=1,  window=8, seed=1337, min_count=5, workers=4,alpha=0.025, min_alpha=0.025)\n",
    "model_sim.build_vocab(doc_sentences)\n",
    "for epoch in range(20):\n",
    "    print(\"epoch \"+str(epoch))\n",
    "    model_sim.train(doc_sentences, total_examples=count, epochs=1)\n",
    "    model_sim.save('commonSense_sim.model')\n",
    "    model_sim.alpha -= 0.002  # decrease the learning rate\n",
    "    model_sim.min_alpha = model_sim.alpha  # fix the learning rate, no decay\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring Vectors and Answering Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_answer_similarity(question, answers, model_sim):\n",
    "    question_vector = model_sim.infer_vector(question)\n",
    "    answer_scores = {}\n",
    "    for answer in answers:\n",
    "        answer_vector = model_sim.infer_vector(answer['answer_text'])\n",
    "        answer_scores[answer['answer_id']] = pairwise.cosine_similarity(question_vector.reshape(1, -1), answer_vector.reshape(1, -1))\n",
    "    best_answer = max(answer_scores.items(), key=lambda k: k[1])[0]\n",
    "    return best_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94402874"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_answer_similarity(new_train[1][0], new_train[2][0], model_sim)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_v6(data, choose_answer_method, model):\n",
    "    results = []\n",
    "    for instance in data:\n",
    "        for question in instance['questions']:\n",
    "            correct_answers = choose_answer_method(question['question_text'], question['answers'], model)\n",
    "            question_results = [instance['text_id'], question['question_id'], correct_answers]\n",
    "            results.append(question_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.121364710718325"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(run_v6(train_data, choose_answer_similarity, model_sim), ideal_results_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.82282069454288"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(run_v6(dev_data, choose_answer_similarity, model_sim), ideal_results_dev)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
